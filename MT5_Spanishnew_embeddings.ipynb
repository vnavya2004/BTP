{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83dba306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.43.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2.0.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: pandas in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (1.4.0)\n",
      "Requirement already satisfied: multiprocess in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: packaging in /home/abcd/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.7.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.24.3)\n",
      "Requirement already satisfied: xxhash in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.11)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2022.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers[torch] in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.43.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.24.3)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (2022.1.18)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: torch in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: psutil in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (2.0.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (0.33.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: pyyaml in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m124.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (0.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.7.0)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.6.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: safetensors, accelerate\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.2\n",
      "    Uninstalling safetensors-0.4.2:\n",
      "      Successfully uninstalled safetensors-0.4.2\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.33.0\n",
      "    Uninstalling accelerate-0.33.0:\n",
      "      Successfully uninstalled accelerate-0.33.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "finetuner-commons 0.13.7 requires torch~=1.12.0, but you have torch 2.4.0 which is incompatible.\n",
      "finetuner-commons 0.13.7 requires torchvision~=0.13.0, but you have torchvision 0.19.0 which is incompatible.\n",
      "finetuner-commons 0.13.7 requires transformers==4.20.1, but you have transformers 4.43.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.0.1 safetensors-0.4.5\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  # To disable GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for a machine learning model. Whenever it is needed to\n",
    "    implement a new model it should inherit and implement each of its methods.\n",
    "    Each inheritted model might be implemented differently but should respect\n",
    "    the signature of the abstract class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: str) -> None:\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self,\n",
    "            x_train: pd.Series,\n",
    "            y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Abstract fit method that takes training text documents `x_train` and\n",
    "        their labels `y_train` and train a model. `x_dev` and `y_dev` can be\n",
    "        used to obtain cross-validation insights, early stopping, or simply\n",
    "        ignore them.\n",
    "\n",
    "        parameters:\n",
    "            - `x_train` (pd.Series[str]) training text documents.\n",
    "            - `y_train` (pd.Series[int]) training labels.\n",
    "            - `x_dev` (pd.Series[str]) dev text documents.\n",
    "            - `y_dev` (pd.Series[int]) dev labels.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Abstract method to perform classification on samples in `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array[int]) class labels for sample `x`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Abstract method to estimate classification probabilities on samples in\n",
    "        `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array of floats with n classes columns) probability\n",
    "              labels for sample `x`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Save model weights as a pickle python file in `self.output_dir` using\n",
    "        its identifier `self.model_name`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, model_dirpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Load model weights. It takes directory path `model_dirpath` where the\n",
    "        model necessary data is in.\n",
    "\n",
    "        parameters:\n",
    "            - `model_dirpath` (str) Directory path where the model is saved.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d73b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:482: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-11-10 22:10:41.912300: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-10 22:10:43.735185: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 22:10:44.534972: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-10 22:10:48.759740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-10 22:10:48.760069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-10 22:10:48.760094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          TextClassificationPipeline, TrainingArguments,\n",
    "                          Trainer, DataCollatorWithPadding)\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    \"\"\"\n",
    "    Huggingface Transformer model for classification such as BERT, DeBERTa,\n",
    "    RoBERTa, etc.\n",
    "\n",
    "    parameters:\n",
    "        - `output_dir` (str) Directory path where the model outputs will be\n",
    "          recorded. That is weights, predictions, etc.\n",
    "\n",
    "        - `model_name` (str) Identifier of the model. It is used to recognize an\n",
    "          instance of the class. For example, if multiple runs are executed with\n",
    "          different parameters, `model_name` can be used to assign a different\n",
    "          name. Also, when saving an instance of the model, it will create a\n",
    "          directory using this parameters as its name and will be saved in\n",
    "          `output_dir`.\n",
    "\n",
    "        - `huggingface-path` (str) the name of the model in the hub of\n",
    "          huggingface. For example: `bert-base-uncased` or\n",
    "          `microsoft/deberta-v3-large`.\n",
    "\n",
    "        - `checkpoint-path` (str) [optional] path to a huggingface checkpoint\n",
    "        directory containing its configuration.\n",
    "\n",
    "        - `epochs` (int) number of epochs for training the transformer.\n",
    "\n",
    "        - `batch-size` (int) batch size used for training the transformer.\n",
    "\n",
    "        - `random_state` (int) integer number to initialize the random state\n",
    "          during the training process.\n",
    "\n",
    "        - `lr` (float) learning rate for training the transformer.\n",
    "\n",
    "        - `weight-decay` (float) weight decay penalty applied to the\n",
    "          transformer.\n",
    "\n",
    "        - `device` (str) Use `cpu` or `gpu`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 huggingface_path: str = \"google/mt5-base\",\n",
    "                 checkpoint_path: str = None,\n",
    "                 epochs: int = 4,\n",
    "                 batch_size: int = 16,\n",
    "                 random_state: int = 42,\n",
    "                 lr: float = 2e-5,\n",
    "                 weight_decay: float = 0.01,\n",
    "                 num_labels: int = 2,\n",
    "                 output_dir: str = \"./default_output_dir\",\n",
    "                 device: str = \"cpu\") -> None:\n",
    "        super(TransformerModel, self).__init__(output_dir)\n",
    "\n",
    "        # Load model from hugginface hub.\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            huggingface_path,\n",
    "            num_labels=num_labels,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        # Load tokenizer from huggingface hub.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(huggingface_path,\n",
    "                                                  do_lower_case=True)\n",
    "        # Set class attributes.\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.args = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def set_training_args(self):\n",
    "        self.args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            learning_rate=self.lr,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            num_train_epochs=self.epochs,\n",
    "            weight_decay=self.weight_decay,\n",
    "            seed=self.random_state,\n",
    "            #data_seed=self.random_state,\n",
    "            optim=\"adamw_hf\")\n",
    "\n",
    "    def tokenize(self, example: str):\n",
    "        \"\"\"\n",
    "        Tokenize a sentence using the model tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "    def build_loader(self, sentences: pd.Series, labels: pd.Series = None):\n",
    "        \"\"\"\n",
    "        Create a Dataset loader from huggingface tokenizing each sentence.\n",
    "\n",
    "        parameters:\n",
    "            - `sentences` (pd.Series[str])\n",
    "            - `labels` (pd.Series[int])\n",
    "        \"\"\"\n",
    "        dataset_dict = {\"text\": sentences}\n",
    "        if labels is not None:\n",
    "            dataset_dict.update({\"label\": labels})\n",
    "    \n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        return dataset.map(self.tokenize, batched=True)\n",
    "\n",
    "    def fit(self,\n",
    "            x_train: pd.Series,\n",
    "            y_train: pd.Series) -> None:\n",
    "        \"\"\"\n",
    "        Fit method that takes training text documents `x_train` and their labels\n",
    "        `y_train` and train a transformer based model. In this case the `x_dev`\n",
    "        and `y_dev` are used to evaluate the model in each epoch. When saving\n",
    "        the model, train and dev losses are saved too.\n",
    "\n",
    "        parameters:\n",
    "            - `x_train` (pd.Series[str]) training text documents.\n",
    "            - `y_train` (pd.Series[int]) training labels.\n",
    "            - `x_dev` (pd.Series[str]) dev text documents.\n",
    "            - `y_dev` (pd.Series[int]) dev labels.\n",
    "        \"\"\"\n",
    "        self.set_training_args()\n",
    "\n",
    "        # Create data collator.\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer,\n",
    "                                                padding=True)\n",
    "\n",
    "        train_size = int(0.8 * len(x_train))  # 80-20 split\n",
    "        train_sentences, eval_sentences = x_train[:train_size], x_train[train_size:]\n",
    "        train_labels, eval_labels = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "        # Create dataset loaders for train and eval sets.\n",
    "        train_dataset = self.build_loader(sentences=train_sentences, labels=train_labels)\n",
    "        eval_dataset = self.build_loader(sentences=eval_sentences, labels=eval_labels)\n",
    "\n",
    "        # Move huggingface model to the device indicated.\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Instance huggingface Trainer.\n",
    "        self.trainer = Trainer(model=self.model,\n",
    "                               args=self.args,\n",
    "                               train_dataset=train_dataset,\n",
    "                               eval_dataset=eval_dataset,\n",
    "                               tokenizer=self.tokenizer,\n",
    "                               data_collator=data_collator)\n",
    "\n",
    "        # If there is any checkpoint provided, training is resumed from it.\n",
    "        if self.checkpoint_path is not None:\n",
    "            self.trainer.train(self.checkpoint_path)\n",
    "        else:\n",
    "            self.trainer.train()\n",
    "\n",
    "    def predict_proba(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Estimate classification probabilities on samples in `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array of floats with n classes columns) probability\n",
    "              labels for sample `x`.\n",
    "        \"\"\"\n",
    "        # Use text classification pipeline to make predictions.\n",
    "        pipe = TextClassificationPipeline(model=self.model,\n",
    "                                          tokenizer=self.tokenizer,\n",
    "                                          return_all_scores=True,\n",
    "                                          framework=\"pt\")\n",
    "        preds = pipe(x.tolist())\n",
    "        y_prob = np.array([[pred[i][\"score\"] for i in range(self.num_labels)]\n",
    "                           for pred in preds])\n",
    "        return y_prob\n",
    "\n",
    "    def predict(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Perform classification on samples in `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array[int]) class labels for sample `x`.\n",
    "        \"\"\"\n",
    "        y_prob = predict_proba(x)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Save model weights and its configuration in `self.output_dir`. It\n",
    "        follows huggingface save standards so the model can be re-loaded using\n",
    "        huggingface `from_pretrained()` functionality.\n",
    "        \"\"\"\n",
    "        if self.trainer is not None:\n",
    "            os.makedirs(f\"{self.output_dir}/model\", exist_ok=True)\n",
    "            self.trainer.save_model(output_dir=f\"{self.output_dir}/model\")\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"Method ignored. Trying to save model without training it.\"\n",
    "                \"Please use `fit` before `save_model`\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "    def load_model(self, model_dirpath):\n",
    "        \"\"\"\n",
    "        Load model weights. It takes directory path `model_dirpath` where the\n",
    "        model necessary data is in.\n",
    "\n",
    "        parameters:\n",
    "            - `model_dirpath` (str) Directory path where the model is saved.\n",
    "        \"\"\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_dirpath)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dirpath)\n",
    "\n",
    "    def embed(self, x: pd.Series) -> np.array:\n",
    "        inputs = self.tokenizer(x.tolist(),\n",
    "                                truncation=True,\n",
    "                                max_length=256,\n",
    "                                padding=\"max_length\",\n",
    "                                return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # Get the last hidden state\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        # Get only the CLS token for each instance in `x` (the one used for classification).\n",
    "        # cls = last_hidden_states[:, 0, :]\n",
    "\n",
    "        # Detach Pytorch tensor to Numpy array.\n",
    "        return last_hidden_states.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5a7e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                             Tweets  Labels\n",
      "0            NaN  DeberÃ­an eliminar a las malas personas y a lo...       1\n",
      "1            NaN  Ya deja de intentar contarle tus problemas a a...       1\n",
      "2            NaN  La tristeza es lo mÃ¡s fÃ¡cil de ocultar de to...       1\n",
      "3            NaN  De las peores cosas de la depresiÃ³n es que no...       1\n",
      "4            NaN  La soledad es lo Ãºnico constante en mi vida. ...       1\n",
      "...          ...                                                ...     ...\n",
      "2181         NaN  Solo quería decir eso ... me rechazaron 7 vece...       0\n",
      "2182         NaN  ¿Quién quiere unirse a mi discordia, todos vib...       0\n",
      "2183         NaN  Si fuera intolerante a la lactosa, simplemente...       0\n",
      "2184         NaN  Chicos que no me quedan mucho tiempo, pero ten...       0\n",
      "2185         NaN                          quiero charlar, dm me.17m       0\n",
      "\n",
      "[2186 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data from CSV files\n",
    "arabic_data = pd.read_excel(\"spanish.xlsx\")\n",
    "# dev_data = pd.read_csv(\"/kaggle/input/translated-datasets/malayalam_only_dev.csv\")\n",
    "# test_data = pd.read_csv(\"Dataset/Transliterated Only/Tamil/tamil_transliterated_test.csv\")\n",
    "print(arabic_data)\n",
    "\n",
    "arabic_data.columns = ['Unnamed: 0','Tweets', 'Labels']\n",
    "\n",
    "# Remove any leading or trailing spaces from the labels\n",
    "# = arabic_data['label'].str.strip()\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "# Assuming you want to split it into 80% train and 20% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(arabic_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally, you can also split a development/validation set if needed\n",
    "# dev_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Again, ensure labels are stripped of any leading or trailing spaces\n",
    "# train_data['label'] = train_data['label'].str.strip()\n",
    "# test_data['label'] = test_data['label'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e5485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns =  ['','text', 'label']\n",
    "# dev_data.columns = ['text', 'label']\n",
    "test_data.columns =  ['','text', 'label']\n",
    "train_data['label'] = train_data['label']\n",
    "# dev_data['label'] = dev_data['label'].str.strip()\n",
    "test_data['label'] = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae6f080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Step 2: Preprocess the data, separating sentences and labels\n",
    "x_train, y_train = train_data[\"text\"], train_data[\"label\"]\n",
    "# x_dev, y_dev = dev_data[\"text\"], dev_data[\"label\"]\n",
    "x_test, y_test = test_data[\"text\"], test_data[\"label\"]\n",
    "\n",
    "# Step 3: Initialize the TransformerModel\n",
    "model = TransformerModel(huggingface_path=\"xlm-roberta-base\",\n",
    "                         epochs=4,\n",
    "                         batch_size=16,\n",
    "                         random_state=42,\n",
    "                         lr=2e-5,\n",
    "                         weight_decay=0.01,\n",
    "                         num_labels=2)\n",
    "\n",
    "# # Step 4: Train the model on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ea2ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741     1\n",
      "1315    0\n",
      "2051    0\n",
      "930     1\n",
      "1476    0\n",
      "       ..\n",
      "1457    0\n",
      "162     1\n",
      "952     1\n",
      "1091    0\n",
      "859     1\n",
      "Name: label, Length: 438, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18623583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_data['label'] = le.fit_transform(train_data['label'])\n",
    "# dev_data['label'] = le.transform(dev_data['label'])\n",
    "test_data['label'] = le.transform(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97747f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<bound method TransformerModel.tokenize of <__main__.TransformerModel object at 0x7f1df6fe16d0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30c7e6bdfa74ac5ae6e618231c59534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565c3298c86448748be74e698dcd596c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcse210001032\u001b[0m (\u001b[33mcse210001032-iit-indore\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abcd/Navya/wandb/run-20241110_221211-qu5d28pn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/qu5d28pn' target=\"_blank\">./default_output_dir</a></strong> to <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/qu5d28pn' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface/runs/qu5d28pn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='352' max='352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [352/352 46:27, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.083917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>0.082533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.089691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.080179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6870de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output directory where you want to save the model\n",
    "output_dir = \"Models/Spanishnew_xlmr.h5\"  # Replace this with your desired output directory\n",
    "\n",
    "# Set the output_dir in the model instance\n",
    "model.output_dir = output_dir\n",
    "\n",
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b95d3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TransformerModel class\n",
    "model = TransformerModel()\n",
    "\n",
    "# Load the saved model from the specified directory\n",
    "model.load_model(\"Models/Spanishnew_xlmr.h5/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c5af460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1748,)\n",
      "(1748,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|███████████████████████| 55/55 [04:22<00:00,  4.77s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Sample sentences for demonstration\n",
    "sample_sentences = x_train  # Assuming x_train contains your sentences\n",
    "labels = y_train  # Assuming y_train contains your labels\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = (len(sample_sentences) + batch_size - 1) // batch_size\n",
    "\n",
    "# Initialize an empty list to store the embeddings and labels\n",
    "embeddings_list = []\n",
    "label_list = []\n",
    "\n",
    "# Use tqdm to create a progress bar\n",
    "for i in tqdm(range(num_batches), desc=\"Processing batches\", total=num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "\n",
    "    # Get embeddings for the current batch\n",
    "    batch_embeddings = model.embed(sample_sentences[start_idx:end_idx])\n",
    "\n",
    "    # Append the embeddings to the list\n",
    "    embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    # Append the corresponding labels to the label list\n",
    "    label_list.append(labels[start_idx:end_idx])\n",
    "\n",
    "# Concatenate embeddings and labels from all batches into a single numpy array.\n",
    "embeddings_list = np.concatenate(embeddings_list, axis=0)\n",
    "label_list = np.concatenate(label_list, axis=0)\n",
    "\n",
    "# Now, embeddings_list contains the embeddings for all sentences,\n",
    "# and label_list contains the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee289d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define the directory path\n",
    "directory = \"Embeddings/MT5/SPANISHNEW\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5c5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to a file\n",
    "# Embeddings/Muril/Tamil/Transliterated Only/tamil_muril_transliterated_train_embeds.npy\n",
    "np.save(\"Embeddings/MT5/SPANISHNEW/spanishnew_mt5_transliterated_train_embeds.npy\", embeddings_list)\n",
    "np.save(\"Embeddings/MT5/SPANISHNEW/spanishnew_mt5_transliterated_train_labels.npy\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb123c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
