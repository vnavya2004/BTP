{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02966f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied: scikit-learn in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: transformers in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: datasets in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: fsspec in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: multiprocess in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: xxhash in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:482: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-11-12 17:08:33.096794: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 17:08:33.314189: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 17:08:33.375428: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 17:08:34.446126: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-12 17:08:34.446232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-12 17:08:34.446240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweets  labels  \\\n",
      "2740  ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∞‡¶æ‡¶§ ‡¶ú‡¶æ‡¶ó‡¶æ ‡¶ü‡¶æ ‡¶∏‡¶æ‡¶∞‡ßç‡¶•‡¶ï ‡¶Ö‡¶≠‡¶ø‡¶®‡¶®‡ßç‡¶¶‡¶® ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ï‡ßç‡¶∞‡¶ø...       0   \n",
      "3615  ‡¶¶‡ßç‡¶¨‡ßÄ‡¶®‡¶¶‡¶æ‡¶∞ ‡¶∏‡ßç‡¶¨‡¶æ‡¶Æ‡ßÄ ‡¶è‡¶ï‡¶ú‡¶® ‡¶®‡ßá‡¶ï‡¶ï‡¶æ‡¶∞ ‡¶∏‡ßç‡¶§‡ßç‡¶∞‡ßÄ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßÉ‡¶•‡¶ø‡¶¨...       0   \n",
      "2064  ‡¶Ü ‡¶≤‡ßÄ‡¶ó ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ‡¶Ø‡¶º ‡¶è‡¶≤‡ßá ‡ß´ ‡¶ú‡¶ø ‡¶∏‡ßá‡¶¨‡¶æ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶π‡¶¨‡ßá : ‡¶ú...       0   \n",
      "1693  ‡¶Ü‡¶∞‡ßç‡¶ï‡¶ø‡¶ü‡ßá‡¶ï‡ßç‡¶ü ‡¶Ö‡¶¨ ‡¶°‡¶ø‡¶ú‡¶ø‡¶ü‡¶æ‡¶≤ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶§‡¶∞‡ßÅ‡¶£ ‡¶™‡ßç‡¶∞‡¶ú‡¶®‡ßç‡¶Æ‡ßá‡¶∞ ...       0   \n",
      "3240  ‡¶Ü‡¶§‡ßç‡¶Æ ‡¶∏‡¶Æ‡ßç‡¶Æ‡¶æ‡¶®‡¶¨‡ßã‡¶ß‡¶π‡¶æ‡¶∞‡¶æ , ‡¶ö‡¶∞‡¶ø‡¶§‡ßç‡¶∞‡¶π‡¶æ‡¶∞‡¶æ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶æ...       0   \n",
      "...                                                 ...     ...   \n",
      "3632  ‡¶≠‡¶¶‡ßç‡¶∞‡¶≤‡ßã‡¶ï ‡¶®‡¶ø‡¶ï‡ßã‡¶ü‡¶ø‡¶® ‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶∞ ‡¶Ü‡¶∂‡¶æ‡¶Ø‡¶º ‡¶®‡¶ø‡¶ï‡ßã‡¶ü‡¶ø‡¶® ‡¶ó‡¶æ‡¶Æ ‡¶ñ‡¶æ‡¶ì‡¶Ø‡¶º...       1   \n",
      "1847  ‡¶∞‡¶æ‡¶∏‡ßÇ‡¶≤‡ßÅ‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶∏‡¶æ ‡¶á‡¶∞‡¶∂‡¶æ‡¶¶ ‡¶ï‡¶∞‡ßá‡¶® , ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡ßá‡¶ï ‡¶∞‡¶æ‡¶§‡ßá‡¶∞ ‡¶Ø‡¶ñ‡¶®...       0   \n",
      "300   ‡¶∏‡¶ø‡¶≤‡ßá‡¶ü ‡¶∏‡¶ø‡¶ü‡¶ø ‡¶ï‡¶∞‡ßç‡¶™‡ßã‡¶∞‡ßá‡¶∂‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá ‡¶Æ‡ßá‡¶Ø‡¶º‡¶∞ ‡¶™‡¶¶‡ßá ‡¶¨‡¶ø‡¶è‡¶®‡¶™...       0   \n",
      "3448  ‡¶§‡¶æ‡¶≤‡¶æ‡¶ï ‡¶è‡¶ñ‡¶® ‡¶Ö‡¶®‡ßá‡¶ï ‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ ‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶è‡¶ï‡¶ï‡¶æ‡¶≤‡ßÄ‡¶® ‡¶ü‡¶æ‡¶ï‡¶æ ...       0   \n",
      "1475  ‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ‡¶¨‡¶æ‡¶∞‡ßç‡¶∑‡¶ø‡¶ï‡ßÄ‡¶§‡ßá ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ö‡ßÅ‡¶∞‡¶æ‡¶∂‡¶ø‡¶∞ ‡¶Æ‡¶æ ‡¶∏‡ßç‡¶¨...       0   \n",
      "\n",
      "                                         tweets_english  \n",
      "2740  waking up my night is worthwhile congratulatio...  \n",
      "3615  devout husband is the best in the world for a ...  \n",
      "2064  g service will be launched if a league comes t...  \n",
      "1693  happy birthday to sajib wazed joy bhai the arc...  \n",
      "3240  people without selfrespect and character do no...  \n",
      "...                                                 ...  \n",
      "3632  the gentleman started consuming nicotine gum h...  \n",
      "1847  rasulullah sallallahu alayhi wasallam said whe...  \n",
      "300   ariful haque chowdhury bnps nominated candidat...  \n",
      "3448  talaq is now the best way to earn onetime mone...  \n",
      "1475  salutations to the heavenly mahasweta devi the...  \n",
      "\n",
      "[782 rows x 3 columns]\n",
      "                                                 tweets  labels  \\\n",
      "1964  ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡ßá‡¶∞ ‡¶ï‡ßã‡¶®‡ßã ‡¶´‡ßç‡¶Ø‡¶æ‡¶Æ‡¶ø‡¶≤‡¶ø , ‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶¨‡¶æ ‡¶®‡¶æ‡¶á ‡¶∏‡ßá ‡¶è‡¶§‡¶ø‡¶Æ ‡•§ ...       0   \n",
      "2742                                          ‡¶ú‡¶ø‡¶§‡¶ø ‡¶ó‡ßá‡¶õ‡¶ø       0   \n",
      "366   ‡¶Ü‡¶ú ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Æ‡¶¶‡¶ø‡¶® ‡¶∏‡¶¨‡¶æ‡¶á ‡¶¶‡ßÅ‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ...       0   \n",
      "3089  ‡¶®‡¶ø‡¶ú‡ßá‡¶ï‡ßá ‡¶™‡¶æ‡¶≤‡ßç‡¶ü‡¶æ‡¶®‡ßã‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶™‡¶¶‡¶ï‡ßç‡¶∑‡ßá‡¶™ ‡¶π‡¶≤ , ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶∏‡ßç...       0   \n",
      "1422  ‡¶Ø‡¶æ‡¶∞‡¶æ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá‡¶≤ ‡¶∏‡¶æ‡¶Æ‡¶≤‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡¶æ ‡¶§‡¶æ‡¶∞‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ...       1   \n",
      "...                                                 ...     ...   \n",
      "1128  , ‡¶Ö‡¶®‡ßç‡¶§‡¶∞‡ßá ‡¶Ø‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶≤‡¶ø‡¶ñ‡ßá‡¶õ‡¶ø ‡¶™‡ßç‡¶∞‡ßá‡¶Æ ‡¶è‡¶Å‡¶ü‡ßá‡¶õ‡¶ø ‡¶ñ‡¶æ‡¶Æ‡ßá ! ‡¶π‡¶æ...       0   \n",
      "578   ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá ‡¶â‡¶†‡¶ø‡¶Ø‡¶º‡¶æ ‡¶Ü‡¶Æ‡¶ø ‡¶Æ‡¶®‡ßá ‡¶Æ‡¶®‡ßá ‡¶ö‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶á ‡¶è‡¶§‡ßã ‡¶¨‡¶°‡¶º ‡¶π‡¶á‡¶õ‡¶ø ...       1   \n",
      "2953  ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞‡ßá‡¶á ‡¶Ø‡ßá‡¶® ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶õ‡¶ø ‡¶∂‡¶§ ‡¶∞‡ßÇ‡¶™‡ßá ‡¶∂‡¶§‡¶¨‡¶æ‡¶∞ ‡¶ú‡¶®‡¶Æ‡ßá ‡¶ú...       0   \n",
      "1754  ‡¶ó‡¶§ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶ó‡ßá ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ó‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶™‡¶ø‡¶õ‡¶®‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ú‡ßÅ‡¶°‡¶º‡ßá ‡¶ß‡¶æ...       1   \n",
      "3502  ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¶‡¶ø‡¶® ‡¶™‡¶∞ ‡¶Æ‡¶®‡ßá‡¶∞ ‡¶Ü‡¶∂‡¶æ‡¶ü‡¶æ ‡¶™‡ßÇ‡¶∞‡¶£ ‡¶π‡¶≤ ‡•§ ‡¶è‡¶ñ‡¶® ‡¶π‡ßÅ‡¶Æ‡¶æ‡¶Ø‡¶º‡¶® ‡¶∏...       0   \n",
      "\n",
      "                                         tweets_english  \n",
      "1964  bhagya has no family no parents he is an orpha...  \n",
      "2742                                              i won  \n",
      "366   today is my daughters birthday everyone will m...  \n",
      "3089  the first step to changing yourself is to know...  \n",
      "1422  those who cant handle a pandal will handle ben...  \n",
      "...                                                 ...  \n",
      "1128   whose name i have written in the heart of lov...  \n",
      "578   when i wake up in the morning i say to myself ...  \n",
      "2953  i have loved you as if i have been born in hun...  \n",
      "1754  a few days ago i hit the back of a car with a ...  \n",
      "3502  after many days the hope of the heart was fulf...  \n",
      "\n",
      "[783 rows x 3 columns]\n",
      "                                                 tweets  labels  \\\n",
      "520   ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø‡ßá ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡¶ø ‡¶Æ‡¶æ‡¶∞ ‡¶ñ‡¶æ‡¶ö‡ßç‡¶õ‡ßá , ‡¶Ü‡¶∏‡¶æ‡¶Æ‡ßá ‡¶ö‡¶≤‡¶õ‡ßá ...       1   \n",
      "996   ‡¶®‡¶ø‡¶∞‡¶®‡ßç‡¶§‡¶∞ ‡¶ó‡¶£‡¶§‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßá‡¶∞ ‡¶ó‡¶≤‡¶æ ‡¶ü‡¶ø‡¶™‡¶õ‡ßá ‡¶Æ‡¶Æ‡¶§‡¶æ ‡¶¨‡¶®‡ßç‡¶¶‡ßç‡¶Ø‡ßã‡¶™‡¶æ‡¶ß‡ßç‡¶Ø...       0   \n",
      "2413  ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßÄ ‡¶è‡¶≤‡¶æ‡¶ï‡¶æ ‡¶∞‡¶æ‡¶∏‡ßç‡¶§‡¶æ‡¶Ø‡¶º ‡¶ó‡¶æ‡¶°‡¶º‡¶ø ‡¶ö‡¶≤‡¶æ‡¶ö‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá ...       1   \n",
      "2     ‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá ‡¶Æ‡¶®‡ßá ‡¶™‡¶°‡¶º‡¶¨‡ßá ‡¶Ø‡¶ñ‡¶®‡¶ø‡¶á ‡¶ú‡ßç‡¶Ø‡ßã‡¶∏‡ßç‡¶®‡¶æ ‡¶π‡¶æ‡¶∏‡ßá ‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá ‡¶Æ‡¶®...       0   \n",
      "3881  ‡¶™‡ßá‡¶ô‡ßç‡¶ó‡ßÅ‡¶á‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶π‡¶Ø‡¶º ! ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶ï‡¶æ‡¶â‡¶Ø‡¶º‡¶æ ‡¶∞ ‡¶¶‡ßã‡¶∑ ...       1   \n",
      "...                                                 ...     ...   \n",
      "2723  ‡¶§‡ßã‡¶∞ ‡¶ö‡ßã‡¶ñ‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶™‡ßá‡¶∞ ‡¶Ü‡¶ï‡¶æ‡¶∂ ‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶á ‡¶§‡¶æ‡¶á ‡¶â‡¶°‡¶º‡¶§‡ßá ‡¶¨‡¶≤‡¶ø‡¶®‡¶ø ‡¶§‡ßã...       0   \n",
      "3358  ‡¶™‡¶•‡ßá‡¶∞ ‡¶∂‡ßá‡¶∑ ‡¶è‡¶∏‡ßá ‡¶¶‡ßá‡¶ñ‡¶ø ‡¶Ö‡¶®‡ßá‡¶ï ‡¶π‡¶ø‡¶∏‡ßá‡¶¨ ‡¶¨‡¶æ‡¶ï‡ßÄ , ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá ‡¶Ø‡¶æ‡¶π‡¶æ...       0   \n",
      "327   ‡¶™‡ßÅ‡¶≤‡¶ø‡¶∂ ‡¶π‡¶æ‡¶≤‡¶æ‡¶∞‡¶æ ‡¶∞‡¶æ‡¶§‡ßá‡¶ì ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶∞‡ßá ‡¶π...       1   \n",
      "3319  ‡¶Ü‡¶¨‡ßç‡¶¶‡ßÅ‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶∞‡¶æ‡¶É ‡¶¨‡¶∞‡ßç‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®‡¶É ‡¶®‡¶¨‡ßÄ ‡¶∏‡¶æ‡¶≤‡ßç‡¶≤‡¶æ‡¶≤‡ßç‡¶≤‡¶æ‡¶π‡ßÅ...       0   \n",
      "3277  ‡ßß‡ßØ‡ßØ‡ßØ ‡¶∏‡¶æ‡¶≤‡ßá ‡ßß‡ßÆ‡ß´ ‡¶ß‡¶æ‡¶∞‡¶æ‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡¶æ‡¶¨‡ßá ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶¶‡ßá‡¶¨ ‡¶≠‡¶ü‡ßç‡¶ü‡¶æ‡¶ö...       0   \n",
      "\n",
      "                                         tweets_english  \n",
      "520   bengalis are being beaten in different states ...  \n",
      "996   trinamool under the leadership of mamata baner...  \n",
      "2413  they stop the traffic on the roads of the elec...  \n",
      "2     you will remember when josna smiles you will r...  \n",
      "3881  if the penguin is a minister so what is the fa...  \n",
      "...                                                 ...  \n",
      "2723  im not the sky the size of your eyes so i didn...  \n",
      "3358  at the end of the road i see many calculations...  \n",
      "327   even at night the police are harassing ordinar...  \n",
      "3319  narrated abdullah the prophet may god bless hi...  \n",
      "3277  in  in the proposal under article  buddhadev b...  \n",
      "\n",
      "[2349 rows x 3 columns]\n",
      "training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d4e9722d9f498291be09302d6fb9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e1e49979834a0d9beab56eb7498e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcse210001032\u001b[0m (\u001b[33mcse210001032-iit-indore\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abcd/Navya/wandb/run-20241112_170842-6tguylcp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/6tguylcp' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/6tguylcp' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface/runs/6tguylcp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 1:37:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434244</td>\n",
       "      <td>0.787995</td>\n",
       "      <td>0.628866</td>\n",
       "      <td>0.319372</td>\n",
       "      <td>0.423611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.453124</td>\n",
       "      <td>0.813538</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.592179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.617143</td>\n",
       "      <td>0.565445</td>\n",
       "      <td>0.590164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended.\n",
      "Iterative Data Harvesting started.\n",
      "782\n",
      "2396\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 55:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706262</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.579235</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.566845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.937431</td>\n",
       "      <td>0.809706</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.544503</td>\n",
       "      <td>0.582633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.960984</td>\n",
       "      <td>0.803321</td>\n",
       "      <td>0.605714</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.579235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396\n",
      "3010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 19:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.964388</td>\n",
       "      <td>0.807152</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.565445</td>\n",
       "      <td>0.588556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.149329</td>\n",
       "      <td>0.799489</td>\n",
       "      <td>0.611842</td>\n",
       "      <td>0.486911</td>\n",
       "      <td>0.542274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.164517</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.586826</td>\n",
       "      <td>0.513089</td>\n",
       "      <td>0.547486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010\n",
      "3079\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 19:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.261432</td>\n",
       "      <td>0.798212</td>\n",
       "      <td>0.654206</td>\n",
       "      <td>0.366492</td>\n",
       "      <td>0.469799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.373985</td>\n",
       "      <td>0.795658</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.371728</td>\n",
       "      <td>0.470199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.302070</td>\n",
       "      <td>0.798212</td>\n",
       "      <td>0.606452</td>\n",
       "      <td>0.492147</td>\n",
       "      <td>0.543353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative Data Harvesting ended.\n",
      "Final Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239a541163544579a010007ff7b28792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='98' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [98/98 01:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model results:\n",
      "Accuracy: 0.7982\n",
      "Precision: 0.6065\n",
      "Recall: 0.4921\n",
      "F1 Score: 0.5434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                 tweets  labels  \\\n",
       " 2740  ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∞‡¶æ‡¶§ ‡¶ú‡¶æ‡¶ó‡¶æ ‡¶ü‡¶æ ‡¶∏‡¶æ‡¶∞‡ßç‡¶•‡¶ï ‡¶Ö‡¶≠‡¶ø‡¶®‡¶®‡ßç‡¶¶‡¶® ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ï‡ßç‡¶∞‡¶ø...       0   \n",
       " 3615  ‡¶¶‡ßç‡¶¨‡ßÄ‡¶®‡¶¶‡¶æ‡¶∞ ‡¶∏‡ßç‡¶¨‡¶æ‡¶Æ‡ßÄ ‡¶è‡¶ï‡¶ú‡¶® ‡¶®‡ßá‡¶ï‡¶ï‡¶æ‡¶∞ ‡¶∏‡ßç‡¶§‡ßç‡¶∞‡ßÄ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßÉ‡¶•‡¶ø‡¶¨...       0   \n",
       " 2064  ‡¶Ü ‡¶≤‡ßÄ‡¶ó ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ‡¶Ø‡¶º ‡¶è‡¶≤‡ßá ‡ß´ ‡¶ú‡¶ø ‡¶∏‡ßá‡¶¨‡¶æ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶π‡¶¨‡ßá : ‡¶ú...       0   \n",
       " 1693  ‡¶Ü‡¶∞‡ßç‡¶ï‡¶ø‡¶ü‡ßá‡¶ï‡ßç‡¶ü ‡¶Ö‡¶¨ ‡¶°‡¶ø‡¶ú‡¶ø‡¶ü‡¶æ‡¶≤ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶§‡¶∞‡ßÅ‡¶£ ‡¶™‡ßç‡¶∞‡¶ú‡¶®‡ßç‡¶Æ‡ßá‡¶∞ ...       0   \n",
       " 3240  ‡¶Ü‡¶§‡ßç‡¶Æ ‡¶∏‡¶Æ‡ßç‡¶Æ‡¶æ‡¶®‡¶¨‡ßã‡¶ß‡¶π‡¶æ‡¶∞‡¶æ , ‡¶ö‡¶∞‡¶ø‡¶§‡ßç‡¶∞‡¶π‡¶æ‡¶∞‡¶æ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶æ...       0   \n",
       " ...                                                 ...     ...   \n",
       " 2027  ‡¶∏‡ßç‡¶¨‡¶™‡ßç‡¶®‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ß‡ßÇ‡¶≤‡¶ø‡¶∏‡¶æ‡ßé ‡¶π‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶Ü‡¶∞ ‡¶ï‡¶∑‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶Æ‡¶ø‡¶•‡ßç‡¶Ø...       0   \n",
       " 2982  ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨ ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ú‡ßÄ‡¶¨‡¶® ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§ ? ‡¶§‡¶æ...       0   \n",
       " 2941  ‡¶¨‡ßá‡¶∂ ‡ßç‡¶Ø‡¶æ ‡¶Ø‡¶¶‡¶ø ‡¶®‡¶∑‡ßç‡¶ü ‡¶π‡¶Ø‡¶º ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡ßá‡¶® ‡¶§‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶™...       1   \n",
       " 545   ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶®‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶π‡¶æ‡¶∏‡¶ø ‡¶™‡¶æ‡¶Ø‡¶º ‡¶ü‡ßÅ‡¶á‡¶ü‡ßá ‡¶Ü‡¶∏‡¶≤‡ßá , ‡¶Ø‡¶ñ‡¶® ‡¶¶‡ßá‡¶ñ‡¶ø ‡¶ï‡¶ø...       1   \n",
       " 3277  ‡ßß‡ßØ‡ßØ‡ßØ ‡¶∏‡¶æ‡¶≤‡ßá ‡ßß‡ßÆ‡ß´ ‡¶ß‡¶æ‡¶∞‡¶æ‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡¶æ‡¶¨‡ßá ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶¶‡ßá‡¶¨ ‡¶≠‡¶ü‡ßç‡¶ü‡¶æ‡¶ö...       0   \n",
       " \n",
       "                                          tweets_english  \n",
       " 2740  waking up my night is worthwhile congratulatio...  \n",
       " 3615  devout husband is the best in the world for a ...  \n",
       " 2064  g service will be launched if a league comes t...  \n",
       " 1693  happy birthday to sajib wazed joy bhai the arc...  \n",
       " 3240  people without selfrespect and character do no...  \n",
       " ...                                                 ...  \n",
       " 2027  dreams fade to dust and pains are buried behin...  \n",
       " 2982  are you ready to give your life for real they ...  \n",
       " 2941  if the penis is spoiled why does the man go to...  \n",
       " 545   i dont really laugh when i see some people act...  \n",
       " 3277  in  in the proposal under article  buddhadev b...  \n",
       " \n",
       " [3079 rows x 3 columns],\n",
       " {'eval_loss': 1.3020703792572021,\n",
       "  'eval_accuracy': 0.7982120051085568,\n",
       "  'eval_precision': 0.6064516129032258,\n",
       "  'eval_recall': 0.49214659685863876,\n",
       "  'eval_f1': 0.5433526011560694,\n",
       "  'eval_runtime': 84.4161,\n",
       "  'eval_samples_per_second': 9.275,\n",
       "  'eval_steps_per_second': 1.161,\n",
       "  'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pandas scikit-learn transformers torch datasets\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score, recall_score\n",
    "\n",
    "# Step 1: Load and Preprocess Dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from a CSV file and check for required columns.\"\"\"\n",
    "    data = pd.read_excel(file_path)\n",
    "#     data = data.sample(frac=0.04, random_state=42)  # Random 40%\n",
    "\n",
    "    if 'tweets_english' not in data.columns:\n",
    "        raise KeyError(\"The dataset must contain a 'text' column with tweet data.\")\n",
    "    if 'labels' not in data.columns:\n",
    "        print(\"Warning: No 'label' column found. Assuming this is the unlabeled data.\")\n",
    "        data['labels'] = None  # Add a label column with NaN values if missing\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_text(df):\n",
    "    \"\"\"Preprocess text data.\"\"\"\n",
    "    df['tweets_english'] = df['tweets_english'].str.lower()\n",
    "    df['tweets_english'] = df['tweets_english'].str.replace(r\"http\\S+|www\\S+|https\\S+\", '', regex=True)  # remove URLs\n",
    "    df['tweets_english'] = df['tweets_english'].str.replace(r'\\@\\w+|\\#', '', regex=True)  # remove mentions and hashtags\n",
    "    df['tweets_english'] = df['tweets_english'].str.replace(r'[^A-Za-z\\s]', '', regex=True)  # remove non-alphanumeric chars\n",
    "    return df\n",
    "\n",
    "# Step 2: Define Labeled and Unlabeled Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, label_column='labels', labeled_size=0.2, unlabeled_size=0.6, test_size=0.2):\n",
    "    \"\"\"Split dataset into labeled, unlabeled, and test data.\"\"\"\n",
    "    # Ensure the sum of labeled_size, unlabeled_size, and test_size equals 1\n",
    "    assert labeled_size + unlabeled_size + test_size == 1, \"The sizes must sum to 1.\"\n",
    "    \n",
    "    # Treat all data as unlabeled, including the rows with labels\n",
    "    \n",
    "    \n",
    "    # Split the data into train (80%) and test (20%) sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Now, from the train_data (80%), we select 20% as labeled data\n",
    "    labeled_data, unlabeled_data = train_test_split(train_data, test_size=0.75, random_state=42)\n",
    "    \n",
    "    return labeled_data,  test_data , unlabeled_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Initialize Model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    batch['tweets_english'] = [str(item) for item in batch['tweets_english']]  \n",
    "    return tokenizer(batch['tweets_english'], padding='max_length', truncation=True, max_length=128)\n",
    "# Step 4: Training Arguments\n",
    "def setup_training_args():\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Training\n",
    "# ... (your existing code) ...\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Trainin\n",
    "from datasets import Dataset  \n",
    "def train_model(train_data, eval_data, model, training_args):\n",
    "    # Convert DataFrames to Hugging Face Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,  # Use Hugging Face Datasets\n",
    "        eval_dataset=eval_dataset,    # Use Hugging Face Datasets\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "def predict_with_confidence(trainer, data, threshold=0.9, batch_size=16):  # Added batch_size parameter\n",
    "    \"\"\"Predicts labels for unlabeled data with high confidence.\"\"\"\n",
    "    # Check if 'data' has any rows\n",
    "    if data.empty:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    # and filter out any empty strings\n",
    "    valid_tweets = [str(item) for item in data['tweets_english'].tolist() if str(item).strip()]\n",
    "    \n",
    "    # Check if there are any valid tweets\n",
    "    if not valid_tweets:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "    \n",
    "    # Process data in batches to reduce memory usage\n",
    "    all_high_confidence_indices = []\n",
    "    for i in range(0, len(valid_tweets), batch_size):\n",
    "        batch_tweets = valid_tweets[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_tweets, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        high_confidence_indices = (probs.max(dim=1).values > threshold).nonzero(as_tuple=True)[0]\n",
    "        all_high_confidence_indices.extend(high_confidence_indices.cpu().numpy() + i)  # Adjust indices\n",
    "\n",
    "    # If no high confidence predictions, return empty DataFrame\n",
    "    if not all_high_confidence_indices:\n",
    "        return pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    return data.iloc[all_high_confidence_indices]\n",
    "\n",
    "def iterative_harvesting(trainer, labeled_data, unlabeled_data, iterations=1, threshold=0.9):\n",
    "    for iteration in range(iterations):\n",
    "        high_conf_data = predict_with_confidence(trainer, unlabeled_data, threshold)\n",
    "        print(len(labeled_data))\n",
    "        labeled_data = pd.concat([labeled_data, high_conf_data])\n",
    "        print(len(labeled_data))\n",
    "        unlabeled_data = unlabeled_data.drop(high_conf_data.index)\n",
    "        trainer.train()  # Retrain on expanded labeled data\n",
    "    return labeled_data, trainer\n",
    "\n",
    "# Step 7: Zero-Shot Learning Integration (optional)\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = [\"depression\", \"no depression\"]\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Evaluation Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)  # Get the predicted class labels\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "# Main Pipeline Function\n",
    "def evaluate_model(trainer, eval_data):\n",
    "    # Re-tokenize eval_data to ensure consistency\n",
    "#     print(\"a\")\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "#     print(\"b\")\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "#     print(\"c\")\n",
    "    # Run evaluation\n",
    "    final_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "#     print(\"s\")\n",
    "    return final_results\n",
    "\n",
    "def run_pipeline(file_path):\n",
    "    # Load and preprocess data\n",
    "    data = load_data(file_path)\n",
    "    data = preprocess_text(data)\n",
    "\n",
    "    # Define labeled and unlabeled datasets\n",
    "    train_data, eval_data, unlabeled_data = split_data(data)\n",
    "    \n",
    "    print(train_data)\n",
    "    print(eval_data)\n",
    "    print(unlabeled_data)\n",
    "    # Initialize training arguments and trainer\n",
    "    training_args = setup_training_args()\n",
    "    print(\"training started.\")\n",
    "    trainer = train_model(train_data, eval_data, model, training_args)\n",
    "    print(\"training ended.\")\n",
    "    # Iterative Data Harvesting\n",
    "    print(\"Iterative Data Harvesting started.\")\n",
    "    final_labeled_data, final_trainer = iterative_harvesting(trainer, train_data, unlabeled_data)\n",
    "    print(\"Iterative Data Harvesting ended.\")\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"Final Evaluation started.\")\n",
    "    \n",
    "    final_results = evaluate_model(final_trainer, eval_data)\n",
    "    print(\"Final model results:\")\n",
    "    print(f\"Accuracy: {final_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {final_results['eval_precision']:.4f}\")\n",
    "    print(f\"Recall: {final_results['eval_recall']:.4f}\")\n",
    "    print(f\"F1 Score: {final_results['eval_f1']:.4f}\")\n",
    "    return final_labeled_data, final_results\n",
    "\n",
    "# Run the pipeline with your dataset file path\n",
    "file_path = 'Bangla2_translated.xlsx'\n",
    "run_pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc4ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
