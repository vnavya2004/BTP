{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9392bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied: scikit-learn in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: transformers in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: datasets in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: xxhash in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                             Tweets  Labels  \\\n",
      "733          NaN                           El dolor me ha cambiado.       1   \n",
      "1031         NaN  razones por las cuales los anti vaxxers son bu...       0   \n",
      "499          NaN               me gustar√É¬≠a desaparecer por un rato       1   \n",
      "793          NaN  Los antidepresivos son como la religi√É¬≥n, solo...       1   \n",
      "766          NaN  Todo me agobia y sofoca, mejor exiliarme de la...       1   \n",
      "...          ...                                                ...     ...   \n",
      "850          NaN   Al parecer no valgo la pena ser amigo de alguien       1   \n",
      "119          NaN  es como que a veces yo sola me dejo caer\\n\\nes...       1   \n",
      "1609         NaN  Nombres de los 10 principales 1. Bob 2. Wayne ...       0   \n",
      "2092         NaN  D√≠a 6 de mi p√©rdida de peso El d√≠a 6 est√° aqu√≠...       0   \n",
      "884          NaN  Esos momentos en los que tengo ganas de enterr...       1   \n",
      "\n",
      "                                         Tweets_english  \n",
      "733                             the pain has changed me  \n",
      "1031  reasons why anti vaxxers are good   of stupid ...  \n",
      "499               i would like to disappear for a while  \n",
      "793   antidepressants are like religion they only gi...  \n",
      "766   everything overwhelms and suffocates me better...  \n",
      "...                                                 ...  \n",
      "850       apparently im not worth being someones friend  \n",
      "119   its like sometimes i just let myself fall\\n\\ni...  \n",
      "1609  top  names  bob  wayne  yes  filler  his name ...  \n",
      "2092  day  of my weight loss day  is here almost the...  \n",
      "884   those moments when i feel like burying a knife...  \n",
      "\n",
      "[437 rows x 4 columns]\n",
      "      Unnamed: 0                                             Tweets  Labels  \\\n",
      "741          NaN  No encuentro forma de sentirme bien, jamas enc...       1   \n",
      "1315         NaN  Honestamente, lo mejor que he hecho en toda mi...       0   \n",
      "2051         NaN  Cualquiera quiere charlar felicitaciones por l...       0   \n",
      "930          NaN  intento tragar el nudo que tengo en la gargant...       1   \n",
      "1476         NaN  Los maestros que llaman en fr√≠o son los peores...       0   \n",
      "...          ...                                                ...     ...   \n",
      "1457         NaN  Todos los que tienen un ATAR bajo, su Atar no ...       0   \n",
      "162          NaN  Siempre le tuve miedo a llegar al punto de que...       1   \n",
      "952          NaN                Aveces tengo miedo de mi misma!√∞≈∏Àú¬≥       1   \n",
      "1091         NaN  ImmA f ing h√°galo si esta publicaci√≥n va algo ...       0   \n",
      "859          NaN              Nadie me necesita, nadie se preocupa.       1   \n",
      "\n",
      "                                         Tweets_english  \n",
      "741   i cant find a way to feel good i never find a ...  \n",
      "1315  honestly the best thing ive ever done in my en...  \n",
      "2051  anyone want to chat congratulations on ranking...  \n",
      "930   i try to swallow the lump in my throatbut it o...  \n",
      "1476  teachers who cold call are the worst so i buil...  \n",
      "...                                                 ...  \n",
      "1457  everyone who has a low atar your atar means no...  \n",
      "162   i was always afraid of reaching the point of w...  \n",
      "952                       sometimes im afraid of myself  \n",
      "1091  imma f ing do it if this post goes something v...  \n",
      "859                        nobody needs me nobody cares  \n",
      "\n",
      "[438 rows x 4 columns]\n",
      "      Unnamed: 0                                             Tweets  Labels  \\\n",
      "769          NaN  Me siento m√É¬°s horrible de lo normal, ya me ro...       1   \n",
      "252          NaN  Estoy tan acostumbrada a estar mal, que ya no ...       1   \n",
      "1517         NaN  diciendo que estoy aburrido durante 1000 d√≠as ...       0   \n",
      "1318         NaN  No hay raz√≥n para tener miedo de las personas ...       0   \n",
      "1022         NaN  Si tu yo de universo paralelo estuviera hacien...       0   \n",
      "...          ...                                                ...     ...   \n",
      "372          NaN           Casi 20 a√É¬±os y sigo sin sentirme feliz.       1   \n",
      "2023         NaN  Este es Harvard, no una barra de stripper, leg...       0   \n",
      "2183         NaN  Si fuera intolerante a la lactosa, simplemente...       0   \n",
      "2032         NaN  Atenci√≥n a todos los monke tienen un pl√°tano p...       0   \n",
      "1830         NaN  Mi maestra no apreciaba mis memes, hice memes ...       0   \n",
      "\n",
      "                                         Tweets_english  \n",
      "769   i feel more horrible than normal i already bro...  \n",
      "252   im so used to being bad that i dont even try t...  \n",
      "1517         saying im bored for  days of day  im bored  \n",
      "1318  there is no reason to be afraid of lizard peop...  \n",
      "1022  if your parallel universe self was doing the e...  \n",
      "...                                                 ...  \n",
      "372           almost  years and i still dont feel happy  \n",
      "2023  this is harvard not a stripper pole legally bl...  \n",
      "2183  if you were lactose intolerant you would just ...  \n",
      "2032  attention to all the monke they have a banana ...  \n",
      "1830  my teacher didnt appreciate my memes i made me...  \n",
      "\n",
      "[1311 rows x 4 columns]\n",
      "training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329cdb47e9404b5bb51dde7d6889bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5b3c91569148eb95b98c29ea781cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [165/165 1:07:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.940639</td>\n",
       "      <td>0.941463</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>0.936893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.161118</td>\n",
       "      <td>0.952055</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.951049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.155722</td>\n",
       "      <td>0.952055</td>\n",
       "      <td>0.926606</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.950588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended.\n",
      "Iterative Data Harvesting started.\n",
      "437\n",
      "1701\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [165/165 50:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.181986</td>\n",
       "      <td>0.952055</td>\n",
       "      <td>0.951456</td>\n",
       "      <td>0.946860</td>\n",
       "      <td>0.949153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.202351</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.957746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193717</td>\n",
       "      <td>0.963470</td>\n",
       "      <td>0.948357</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.961905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n",
      "1740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90/165 14:38 < 12:28, 0.10 it/s, Epoch 1.62/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.269280</td>\n",
       "      <td>0.947489</td>\n",
       "      <td>0.918182</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.946136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 214>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Run the pipeline with your dataset file path\u001b[39;00m\n\u001b[1;32m    213\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspanish_translated.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 214\u001b[0m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Iterative Data Harvesting\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterative Data Harvesting started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m final_labeled_data, final_trainer \u001b[38;5;241m=\u001b[39m \u001b[43miterative_harvesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterative Data Harvesting ended.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Final Evaluation\u001b[39;00m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36miterative_harvesting\u001b[0;34m(trainer, labeled_data, unlabeled_data, iterations, threshold)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labeled_data))\n\u001b[1;32m    145\u001b[0m     unlabeled_data \u001b[38;5;241m=\u001b[39m unlabeled_data\u001b[38;5;241m.\u001b[39mdrop(high_conf_data\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m--> 146\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Retrain on expanded labeled data\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labeled_data, trainer\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/trainer.py:2341\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2339\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m-> 2341\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2345\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/accelerate/optimizer.py:171\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/adamw.py:227\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         state_steps,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/adamw.py:767\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 767\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/optim/adamw.py:380\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    377\u001b[0m param\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pandas scikit-learn transformers torch datasets\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score, recall_score\n",
    "\n",
    "# Step 1: Load and Preprocess Dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from a CSV file and check for required columns.\"\"\"\n",
    "    data = pd.read_excel(file_path)\n",
    "#     data = data.sample(frac=0.04, random_state=42)  # Random 40%\n",
    "\n",
    "    if 'Tweets_english' not in data.columns:\n",
    "        raise KeyError(\"The dataset must contain a 'text' column with tweet data.\")\n",
    "    if 'Labels' not in data.columns:\n",
    "        print(\"Warning: No 'label' column found. Assuming this is the unlabeled data.\")\n",
    "        data['Labels'] = None  # Add a label column with NaN values if missing\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_text(df):\n",
    "    \"\"\"Preprocess text data.\"\"\"\n",
    "    df['Tweets_english'] = df['Tweets_english'].str.lower()\n",
    "    df['Tweets_english'] = df['Tweets_english'].str.replace(r\"http\\S+|www\\S+|https\\S+\", '', regex=True)  # remove URLs\n",
    "    df['Tweets_english'] = df['Tweets_english'].str.replace(r'\\@\\w+|\\#', '', regex=True)  # remove mentions and hashtags\n",
    "    df['Tweets_english'] = df['Tweets_english'].str.replace(r'[^A-Za-z\\s]', '', regex=True)  # remove non-alphanumeric chars\n",
    "    return df\n",
    "\n",
    "# Step 2: Define Labeled and Unlabeled Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, label_column='Labels', labeled_size=0.2, unlabeled_size=0.6, test_size=0.2):\n",
    "    \"\"\"Split dataset into labeled, unlabeled, and test data.\"\"\"\n",
    "    # Ensure the sum of labeled_size, unlabeled_size, and test_size equals 1\n",
    "    assert labeled_size + unlabeled_size + test_size == 1, \"The sizes must sum to 1.\"\n",
    "    \n",
    "    # Treat all data as unlabeled, including the rows with labels\n",
    "    \n",
    "    \n",
    "    # Split the data into train (80%) and test (20%) sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Now, from the train_data (80%), we select 20% as labeled data\n",
    "    labeled_data, unlabeled_data = train_test_split(train_data, test_size=0.75, random_state=42)\n",
    "    \n",
    "    return labeled_data,  test_data , unlabeled_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Initialize Model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    batch['Tweets_english'] = [str(item) for item in batch['Tweets_english']]  \n",
    "    return tokenizer(batch['Tweets_english'], padding='max_length', truncation=True, max_length=128)\n",
    "# Step 4: Training Arguments\n",
    "def setup_training_args():\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Training\n",
    "# ... (your existing code) ...\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Trainin\n",
    "from datasets import Dataset  \n",
    "def train_model(train_data, eval_data, model, training_args):\n",
    "    train_data = train_data.rename(columns={'Labels': 'label'})\n",
    "    eval_data = eval_data.rename(columns={'Labels': 'label'})\n",
    "    train_data['label'] = train_data['label'].astype(int)\n",
    "    eval_data['label'] = eval_data['label'].astype(int)\n",
    "    # Convert DataFrames to Hugging Face Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,  # Use Hugging Face Datasets\n",
    "        eval_dataset=eval_dataset,    # Use Hugging Face Datasets\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "def predict_with_confidence(trainer, data, threshold=0.9, batch_size=16):  # Added batch_size parameter\n",
    "    \"\"\"Predicts labels for unlabeled data with high confidence.\"\"\"\n",
    "    # Check if 'data' has any rows\n",
    "    if data.empty:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    # and filter out any empty strings\n",
    "    valid_tweets = [str(item) for item in data['Tweets_english'].tolist() if str(item).strip()]\n",
    "    \n",
    "    # Check if there are any valid tweets\n",
    "    if not valid_tweets:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "    \n",
    "    # Process data in batches to reduce memory usage\n",
    "    all_high_confidence_indices = []\n",
    "    for i in range(0, len(valid_tweets), batch_size):\n",
    "        batch_tweets = valid_tweets[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_tweets, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        high_confidence_indices = (probs.max(dim=1).values > threshold).nonzero(as_tuple=True)[0]\n",
    "        all_high_confidence_indices.extend(high_confidence_indices.cpu().numpy() + i)  # Adjust indices\n",
    "\n",
    "    # If no high confidence predictions, return empty DataFrame\n",
    "    if not all_high_confidence_indices:\n",
    "        return pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    return data.iloc[all_high_confidence_indices]\n",
    "\n",
    "def iterative_harvesting(trainer, labeled_data, unlabeled_data, iterations=1, threshold=0.9):\n",
    "    for iteration in range(iterations):\n",
    "        high_conf_data = predict_with_confidence(trainer, unlabeled_data, threshold)\n",
    "        print(len(labeled_data))\n",
    "        labeled_data = pd.concat([labeled_data, high_conf_data])\n",
    "        print(len(labeled_data))\n",
    "        unlabeled_data = unlabeled_data.drop(high_conf_data.index)\n",
    "        trainer.train()  # Retrain on expanded labeled data\n",
    "    return labeled_data, trainer\n",
    "\n",
    "# Step 7: Zero-Shot Learning Integration (optional)\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "\n",
    "# Step 8: Evaluation Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)  # Get the predicted class labels\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "# Main Pipeline Function\n",
    "def evaluate_model(trainer, eval_data):\n",
    "    # Re-tokenize eval_data to ensure consistency\n",
    "#     print(\"a\")\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "#     print(\"b\")\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "#     print(\"c\")\n",
    "    # Run evaluation\n",
    "    final_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "#     print(\"s\")\n",
    "    return final_results\n",
    "\n",
    "def run_pipeline(file_path):\n",
    "    # Load and preprocess data\n",
    "    data = load_data(file_path)\n",
    "    data = preprocess_text(data)\n",
    "\n",
    "    # Define labeled and unlabeled datasets\n",
    "    train_data, eval_data, unlabeled_data = split_data(data)\n",
    "    \n",
    "    print(train_data)\n",
    "    print(eval_data)\n",
    "    print(unlabeled_data)\n",
    "    # Initialize training arguments and trainer\n",
    "    training_args = setup_training_args()\n",
    "    print(\"training started.\")\n",
    "    trainer = train_model(train_data, eval_data, model, training_args)\n",
    "    print(\"training ended.\")\n",
    "    # Iterative Data Harvesting\n",
    "    print(\"Iterative Data Harvesting started.\")\n",
    "    final_labeled_data, final_trainer = iterative_harvesting(trainer, train_data, unlabeled_data)\n",
    "    print(\"Iterative Data Harvesting ended.\")\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"Final Evaluation started.\")\n",
    "    \n",
    "    final_results = evaluate_model(final_trainer, eval_data)\n",
    "    print(\"Final model results:\")\n",
    "    print(f\"Accuracy: {final_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {final_results['eval_precision']:.4f}\")\n",
    "    print(f\"Recall: {final_results['eval_recall']:.4f}\")\n",
    "    print(f\"F1 Score: {final_results['eval_f1']:.4f}\")\n",
    "    return final_labeled_data, final_results\n",
    "\n",
    "# Run the pipeline with your dataset file path\n",
    "file_path = 'spanish_translated.xlsx'\n",
    "run_pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656baeae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
