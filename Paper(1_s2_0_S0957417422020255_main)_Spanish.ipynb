{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vnavya2004/BTP/blob/main/Paper(1_s2_0_S0957417422020255_main)_Spanish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install numpy pandas tensorflow keras gensim nltk scikit-learn openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXvcWEH9m43S",
        "outputId": "97cfa400-dcf8-444f-fbda-90c5d0dc9727"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Conv1D, MaxPooling1D, Dense, Dropout, Input, Bidirectional, Attention, GlobalAveragePooling1D, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY056H8Em_rF",
        "outputId": "b39473b4-cd04-4ac5-cddb-15e01999cb76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 3: Load and Preprocess Data\n",
        "# Load the Excel file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Manually upload your .xlsx file\n",
        "\n",
        "# Change the file name as per the uploaded file\n",
        "df = pd.read_excel(next(iter(uploaded.keys())))  # Replace with 'filename.xlsx' if needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "yux1sDQfnGFf",
        "outputId": "1bb72185-e350-4681-d4da-d94acd22a7bf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c5dfb7af-a1a7-4887-b425-3ee38797ad1d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c5dfb7af-a1a7-4887-b425-3ee38797ad1d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving spanish_translated_bangla.xlsx to spanish_translated_bangla.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df.sample(frac=0.4, random_state=42)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKrjcA8FnRIX",
        "outputId": "d3d413d6-e38c-4e5e-a8ab-a4660298c544"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                             Tweets  Labels  \\\n",
            "0         NaN  DeberÃ­an eliminar a las malas personas y a lo...       1   \n",
            "1         NaN  Ya deja de intentar contarle tus problemas a a...       1   \n",
            "2         NaN  La tristeza es lo mÃ¡s fÃ¡cil de ocultar de to...       1   \n",
            "3         NaN  De las peores cosas de la depresiÃ³n es que no...       1   \n",
            "4         NaN  La soledad es lo Ãºnico constante en mi vida. ...       1   \n",
            "\n",
            "                                       Tweets_bangla  \n",
            "0  তাদের উচিত খারাপ লোকদের এবং যারা বাঁচতে ঘৃণা ক...  \n",
            "1  আপনার সমস্যা কাউকে বলার চেষ্টা করা বন্ধ করুন। ...  \n",
            "2  বিষণ্নতা তার সাথে নিয়ে আসে এমন সমস্ত অনুভূতির...  \n",
            "3  বিষণ্নতা সম্পর্কে সবচেয়ে খারাপ জিনিসগুলির মধ্...  \n",
            "4  একাকীত্ব আমার জীবনের একমাত্র ধ্রুবক জিনিস। আমি...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the text and label columns\n",
        "text_column_name = 'Tweets_bangla'  # replace with actual column name for text\n",
        "label_column_name = 'Labels'  # replace with actual column name for labels\n"
      ],
      "metadata": {
        "id": "98Zlsw6WnUzh"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(str(text).lower())\n",
        "    words = [word for word in tokens if word.isalpha()]\n",
        "    words = [word for word in words if word not in stopwords.words('bengali')]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[text_column_name] = df[text_column_name].apply(preprocess_text)\n",
        "\n",
        "# Step 4: Split Data into Train and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[text_column_name], df[label_column_name], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0pj5Tu2TngBN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NkoTjfMHrjXv",
        "outputId": "c4f70444-6b6d-40ae-abe9-cbc2e4cba4b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fasttext_model = gensim.models.KeyedVectors.load('/content/drive/MyDrive/embeddings/fasttext_bn_model.kv')\n"
      ],
      "metadata": {
        "id": "0pEykOawsH0c"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from google.colab import drive\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# # Load the GloVe embeddings\n",
        "# glove_model_path = '/content/drive/MyDrive/bn_glove.39M.300d.txt'\n",
        "# word2vec_output_file = '/content/GloVe-Bengali/word2vec_bengali.txt'\n",
        "\n",
        "# # Convert GloVe format to Word2Vec format\n",
        "# glove2word2vec(glove_model_path, word2vec_output_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "MI-cNDa1Q_tA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n"
      ],
      "metadata": {
        "id": "u0So0hFiQ__Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Save the model to Google Drive in binary format\n",
        "# glove_model_save_path = '/content/drive/MyDrive/embeddings/glove_bn_model.kv'\n",
        "# os.makedirs(os.path.dirname(glove_model_save_path), exist_ok=True)\n",
        "\n",
        "# glove_model.save(glove_model_save_path)"
      ],
      "metadata": {
        "id": "_o7Bu9qiYVVX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model_path = '/content/drive/MyDrive/embeddings/glove_bn_model.kv'\n",
        "glove_model = KeyedVectors.load(glove_model_path)"
      ],
      "metadata": {
        "id": "1bNjFrqLYmqK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/sagorbrur/bnlp.git\n",
        "# #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym79mSKnYqGD",
        "outputId": "14adef83-7373-4939-b898-43a3f7804b4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bnlp'...\n",
            "remote: Enumerating objects: 2130, done.\u001b[K\n",
            "remote: Counting objects: 100% (653/653), done.\u001b[K\n",
            "remote: Compressing objects: 100% (322/322), done.\u001b[K\n",
            "remote: Total 2130 (delta 387), reused 535 (delta 326), pack-reused 1477 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2130/2130), 22.76 MiB | 26.10 MiB/s, done.\n",
            "Resolving deltas: 100% (1265/1265), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bnlp_toolkit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6hyo2jUHcm5c",
        "outputId": "c54a500b-5af3-47d1-a933-85f27396d23c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-4.0.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (0.2.0)\n",
            "Collecting gensim==4.3.2 (from bnlp_toolkit)\n",
            "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (1.26.4)\n",
            "Collecting scipy==1.10.1 (from bnlp_toolkit)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn-crfsuite==0.3.6 (from bnlp_toolkit)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting tqdm==4.66.3 (from bnlp_toolkit)\n",
            "  Downloading tqdm-4.66.3-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.2.0 (from bnlp_toolkit)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting emoji==1.7.0 (from bnlp_toolkit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (2.32.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.2.0->bnlp_toolkit) (0.2.13)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2->bnlp_toolkit) (7.0.5)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite==0.3.6->bnlp_toolkit)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2024.8.30)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.2->bnlp_toolkit) (1.16.0)\n",
            "Downloading bnlp_toolkit-4.0.3-py3-none-any.whl (22 kB)\n",
            "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Downloading tqdm-4.66.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=abf60055a10b156168e1b94df5d796a5b1a810fc05b73854a4f69a697374be02\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, tqdm, scipy, python-crfsuite, ftfy, sklearn-crfsuite, gensim, bnlp_toolkit\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "Successfully installed bnlp_toolkit-4.0.3 emoji-1.7.0 ftfy-6.2.0 gensim-4.3.2 python-crfsuite-0.9.11 scipy-1.10.1 sklearn-crfsuite-0.3.6 tqdm-4.66.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              },
              "id": "29f573558e074037bf70bca159edad0a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import BengaliWord2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Initialize BengaliWord2Vec with pretrained model\n",
        "bwv = BengaliWord2Vec()\n",
        "\n",
        "# Define function to retrieve word vector embeddings for BengaliWord2Vec\n",
        "def get_bwv_embedding_matrix(word_index):\n",
        "    \"\"\"\n",
        "    Creates an embedding matrix for the BengaliWord2Vec model.\n",
        "\n",
        "    Args:\n",
        "        word_index (dict): A dictionary mapping words to their indices.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The embedding matrix.\n",
        "    \"\"\"\n",
        "    embedding_dim = 100  # Assuming a dimension of 300 for BengaliWord2Vec\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            vector = bwv.get_word_vector(word)\n",
        "            if vector is not None and len(vector) > 0:\n",
        "                embedding_matrix[i] = vector\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Word '{word}' not found in the BengaliWord2Vec vocabulary. Skipping.\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# Define function to retrieve word vector embeddings for other models\n",
        "def get_embedding_matrix(word_index, embedding_model, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Creates an embedding matrix for a given embedding model.\n",
        "\n",
        "    Args:\n",
        "        word_index (dict): A dictionary mapping words to their indices.\n",
        "        embedding_model (object): The word embedding model.\n",
        "        embedding_dim (int): The desired embedding dimension.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The embedding matrix.\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if word in embedding_model:\n",
        "            embedding_matrix[i] = embedding_model[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "# Tokenize the input data\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "padded_train = pad_sequences(train_sequences, maxlen=100)\n",
        "padded_test = pad_sequences(test_sequences, maxlen=100)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Step 4: Create embedding matrices for each model\n",
        "embedding_matrix_bwv = get_bwv_embedding_matrix(word_index)\n",
        "embedding_matrix_ft = get_embedding_matrix(word_index, fasttext_model, embedding_dim=300)\n",
        "embedding_matrix_glove = get_embedding_matrix(word_index, glove_model, embedding_dim=300)\n",
        "\n",
        "# Step 5: Build the Hybrid Model\n",
        "# Input layer\n",
        "input_layer = Input(shape=(100,))\n",
        "\n",
        "# Embedding layers for each embedding type\n",
        "embedding_layer_bwv = Embedding(len(word_index) + 1, 100, weights=[embedding_matrix_bwv], trainable=False)(input_layer)\n",
        "embedding_layer_ft = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix_ft], trainable=False)(input_layer)\n",
        "embedding_layer_glove = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix_glove], trainable=False)(input_layer)\n",
        "\n",
        "# Concatenate embeddings\n",
        "concatenated_embeddings = Concatenate()([embedding_layer_bwv, embedding_layer_ft, embedding_layer_glove])\n",
        "\n",
        "\n",
        "# Continue building the model as needed...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzLtPDMilTKl",
        "outputId": "c85a1c9b-c7e9-44fa-fb96-3f896f343cc8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Word 'amp' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'পগ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'google' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'reddit' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'idk' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lt' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dm' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'gt' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'meets' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'gf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'goo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ðÿ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'youtube' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'rn' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'wtf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ooga' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'booga' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'subreddit' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'smh' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'idc' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'উঠব' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'spotify' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lol' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'minecraft' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'fr' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lego' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'relapses' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'উফ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ur' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'mf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'rvy' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'wryyy' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'pog' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tbh' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lmao' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nsfw' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lpm' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tldr' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'af' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ik' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ffs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'xxx' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'femboys' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'এনএনএন' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'rapunzel' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bakugan' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'yo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nordvpn' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'mlg' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'll' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'youtooz' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'amc' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bach' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'anime' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ist' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'slutty' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'উওওওওওওওওওওও' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'doom' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'meathooks' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'alt' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'scp' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'আআআআ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tlldr' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'আইআরএল' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ankylosaur' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'cedric' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'suckers' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ama' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dmmmmm' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'osu' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'osucon' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'smth' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'cuptoast' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tmy' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'amps' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'goooooo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ich' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'knnte' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'enfach' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nvm' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'crush' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'gfs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sbc' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dvdu' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'rv' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'jhdc' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dyg' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ygec' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sejc' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'njxa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'djdiddo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'yooo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'milf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'gooo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'উহহহ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ইদক' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'upv' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaah' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aah' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'vrgtenunnumunrunrunrenrewvwcegefegtngmfbrnebebrnrjrmrmebdbwbwybywnynywne' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'wgole' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'smthn' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'আআআআহ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'vulfpeck' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'mfs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nooo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hr' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'relationadvice' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'xx' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'yt' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'উহহহহ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'chad' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'yearpark' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'eenie' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'meenie' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'mo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dnd' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ffffffffiiiiiiiillllllllleeeeeeeeeeeeeeee' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'motions' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'airpods' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'att' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'subreddits' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'adhdb' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'vicks' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'vaporub' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'cocks' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'jk' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hairstyle' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'oooooooooooooooooooooooooooooh' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tw' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'cs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dare' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'img' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ptm' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'shh' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'oeuf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bmi' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'wvclmf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ryzen' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'rtx' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'xbox' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ayo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'psa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hw' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'leeltererci' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ª' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'হপস' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tfw' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'pcvr' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'koahdmxibwbw' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'vibe' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dntst' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'এসক' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hopz' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hip' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hop' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'prime' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ish' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lofi' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'chay' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sobrada' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'caf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'vibez' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'আউড' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'scream' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aaaaaaaaaaaaó' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tiktok' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'xoncqq' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ppppie' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'docsnow' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'oompa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'loompa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'heeeell' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'pppppp' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nasa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'eva' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'asf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'idkk' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'বলছ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'fucked' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'io' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ps' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'imma' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'applebee' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'irl' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'beyblades' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'fsb' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bruh' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tik' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aquaman' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lgbt' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'waddup' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'traps' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hawt' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'arthur' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'meme' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'noice' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bingus' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'floppa' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sogga' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ngl' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'randonautica' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'memes' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'arcaea' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'djaodncwkcjfowjf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'eoenfsowkfn' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'mfw' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'frik' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dysphoria' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sucks' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'smtg' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'fap' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'btw' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'dms' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'froggy' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ahhhg' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ydc' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'linkkk' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'goola' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'oo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'balooga' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'booo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ofc' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nerf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sux' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'bfs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nana' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'lgbtq' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'আএএএএ' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'njggers' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'smite' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ticstourette' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'zelda' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'minecrafter' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'mods' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'htmwmv' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'psuedo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'aud' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'abs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'automod' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'goethoven' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'symphony' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'beethoven' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'elise' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'violin' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sonata' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'cello' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'nicolo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'paganinis' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'caprices' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'imo' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'meeeeeeeeeee' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'hehe' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'whehhwwww' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ggs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'boobs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'ups' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'welp' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'docs' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'slides' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sheet' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'contest' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'maker' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'presentation' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'documents' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'pupper' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'jacobdoeslife' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'wahhhhh' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'tinder' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'theechland' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'danstagt' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'auf' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'setrummern' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'alten' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'gesellschaft' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sidneys' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'sid' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'teens' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'rafikebwocnekdbwkidbns' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'degrime' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'cerel' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'idkkkkkk' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'wudewedh' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'uedjeui' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'djewhu' not found in the BengaliWord2Vec vocabulary. Skipping.\n",
            "Warning: Word 'mwah' not found in the BengaliWord2Vec vocabulary. Skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(concatenated_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Venzgci0piX4",
        "outputId": "cbe7697e-1fac-4757-dc2a-2ffdf8f61148"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<KerasTensor shape=(None, 100, 700), dtype=float32, sparse=False, name=keras_tensor_48>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dimension for each model to ensure compatibility\n",
        "fasttext_dim = len(fasttext_model[next(iter(fasttext_model.key_to_index))])  # For gensim FastText model\n",
        "glove_dim = len(glove_model[next(iter(glove_model.key_to_index))])          # For gensim GloVe model\n",
        "\n",
        "# Verify dimensions\n",
        "print(f\"FastText embedding dimension: {fasttext_dim}\")\n",
        "print(f\"GloVe embedding dimension: {glove_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plbIbh0houQn",
        "outputId": "9aed422f-8340-48c8-ad7c-75a41be0aa8b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText embedding dimension: 300\n",
            "GloVe embedding dimension: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Attention\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "\n",
        "# Assuming `input_layer` and `concatenated_embeddings` are already defined\n",
        "# Layer definitions\n",
        "bilstm_out = Bidirectional(LSTM(100, return_sequences=True))(concatenated_embeddings)\n",
        "lstm_out = LSTM(100, return_sequences=True)(bilstm_out)\n",
        "cnn_out = Conv1D(filters=50, kernel_size=3, activation='relu')(lstm_out)\n",
        "\n",
        "attention_out = Attention()([cnn_out, cnn_out])  # Adjusted for attention layer\n",
        "\n",
        "# Global max pooling and dense layers\n",
        "max_pool_out = GlobalMaxPooling1D()(attention_out)  # Applying GlobalMaxPooling1D after attention\n",
        "dropout_layer_1 = Dropout(0.5)(max_pool_out)\n",
        "dense_layer_1 = Dense(250, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(0.5)(dense_layer_1)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer_2)  # Use sigmoid for binary classification\n",
        "\n",
        "# Define and compile model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Use binary crossentropy for binary classification\n",
        "model.summary()\n",
        "\n",
        "# Step 8: Train the Model\n",
        "model.fit(padded_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Step 9: Evaluate the Model\n",
        "predictions = model.predict(padded_test)\n",
        "predicted_classes = (predictions > 0.5).astype(\"int32\")  # Get the predicted class indices based on a threshold\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_classes))\n",
        "print(\"AUC:\", roc_auc_score(y_test, predictions))  # For binary classification, use the predictions directly\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predicted_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_JfOybpA1eUe",
        "outputId": "3a41fb8b-3d55-4c7c-9761-8c021a919b7e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │         \u001b[38;5;34m53,300\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │        \u001b[38;5;34m159,900\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │        \u001b[38;5;34m159,900\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m700\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ embedding_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ embedding_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │                        │                │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │        \u001b[38;5;34m640,800\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │        \u001b[38;5;34m120,400\u001b[0m │ bidirectional_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │         \u001b[38;5;34m15,050\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_3 (\u001b[38;5;33mAttention\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ attention_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │         \u001b[38;5;34m12,750\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m251\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">53,300</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">159,900</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">159,900</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ embedding_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │                        │                │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640,800</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">120,400</span> │ bidirectional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15,050</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12,750</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">251</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,162,351\u001b[0m (4.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,162,351</span> (4.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m789,251\u001b[0m (3.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">789,251</span> (3.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m373,100\u001b[0m (1.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">373,100</span> (1.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.5304 - loss: 0.6773 - val_accuracy: 0.6457 - val_loss: 0.5593\n",
            "Epoch 2/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6564 - loss: 0.6107 - val_accuracy: 0.6171 - val_loss: 0.5653\n",
            "Epoch 3/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6603 - loss: 0.6114 - val_accuracy: 0.6171 - val_loss: 0.5621\n",
            "Epoch 4/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6289 - loss: 0.5955 - val_accuracy: 0.5943 - val_loss: 0.5911\n",
            "Epoch 5/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6457 - loss: 0.5794 - val_accuracy: 0.5886 - val_loss: 0.5968\n",
            "Epoch 6/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6740 - loss: 0.5639 - val_accuracy: 0.5714 - val_loss: 0.6180\n",
            "Epoch 7/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.6621 - loss: 0.5724 - val_accuracy: 0.5829 - val_loss: 0.6055\n",
            "Epoch 8/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6586 - loss: 0.5659 - val_accuracy: 0.5886 - val_loss: 0.6580\n",
            "Epoch 9/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6544 - loss: 0.5608 - val_accuracy: 0.5657 - val_loss: 0.6045\n",
            "Epoch 10/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6735 - loss: 0.5570 - val_accuracy: 0.6000 - val_loss: 0.6324\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Accuracy: 0.6438356164383562\n",
            "AUC: 0.6983290461551331\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.34      0.50       231\n",
            "           1       0.57      0.99      0.72       207\n",
            "\n",
            "    accuracy                           0.64       438\n",
            "   macro avg       0.77      0.66      0.61       438\n",
            "weighted avg       0.78      0.64      0.61       438\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwiOq6MG3e59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}