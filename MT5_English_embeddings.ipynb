{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dc2734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.38.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abcd/.local/lib/python3.8/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pandas in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (1.4.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: aiohttp in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: packaging in /home/abcd/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: responses<0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abcd/.local/lib/python3.8/site-packages (from datasets) (1.23.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.11)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers[torch] in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.38.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (2022.1.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (4.62.3)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abcd/.local/lib/python3.8/site-packages (from transformers[torch]) (1.23.1)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (3.7.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (0.28.0)\n",
      "Requirement already satisfied: torch in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: psutil in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.7.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.7.91)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch->transformers[torch]) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->transformers[torch]) (61.2.0)\n",
      "Requirement already satisfied: wheel in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->transformers[torch]) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from triton==2.0.0->torch->transformers[torch]) (3.27.1)\n",
      "Requirement already satisfied: lit in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from triton==2.0.0->torch->transformers[torch]) (16.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers[torch]) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (0.28.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: psutil in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: pyyaml in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abcd/.local/lib/python3.8/site-packages (from accelerate) (1.23.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.7.0)\n",
      "Requirement already satisfied: setuptools in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (61.2.0)\n",
      "Requirement already satisfied: wheel in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.1)\n",
      "Requirement already satisfied: lit in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.62.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2022.6.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  # To disable GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for a machine learning model. Whenever it is needed to\n",
    "    implement a new model it should inherit and implement each of its methods.\n",
    "    Each inheritted model might be implemented differently but should respect\n",
    "    the signature of the abstract class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: str) -> None:\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self,\n",
    "            x_train: pd.Series,\n",
    "            y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Abstract fit method that takes training text documents `x_train` and\n",
    "        their labels `y_train` and train a model. `x_dev` and `y_dev` can be\n",
    "        used to obtain cross-validation insights, early stopping, or simply\n",
    "        ignore them.\n",
    "\n",
    "        parameters:\n",
    "            - `x_train` (pd.Series[str]) training text documents.\n",
    "            - `y_train` (pd.Series[int]) training labels.\n",
    "            - `x_dev` (pd.Series[str]) dev text documents.\n",
    "            - `y_dev` (pd.Series[int]) dev labels.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Abstract method to perform classification on samples in `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array[int]) class labels for sample `x`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Abstract method to estimate classification probabilities on samples in\n",
    "        `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array of floats with n classes columns) probability\n",
    "              labels for sample `x`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Save model weights as a pickle python file in `self.output_dir` using\n",
    "        its identifier `self.model_name`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, model_dirpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Load model weights. It takes directory path `model_dirpath` where the\n",
    "        model necessary data is in.\n",
    "\n",
    "        parameters:\n",
    "            - `model_dirpath` (str) Directory path where the model is saved.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8e2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 12:33:22.608578: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 12:33:22.789308: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 12:33:22.820984: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 12:33:23.460780: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-03-29 12:33:23.460864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-03-29 12:33:23.460871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          TextClassificationPipeline, TrainingArguments,\n",
    "                          Trainer, DataCollatorWithPadding)\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    \"\"\"\n",
    "    Huggingface Transformer model for classification such as BERT, DeBERTa,\n",
    "    RoBERTa, etc.\n",
    "\n",
    "    parameters:\n",
    "        - `output_dir` (str) Directory path where the model outputs will be\n",
    "          recorded. That is weights, predictions, etc.\n",
    "\n",
    "        - `model_name` (str) Identifier of the model. It is used to recognize an\n",
    "          instance of the class. For example, if multiple runs are executed with\n",
    "          different parameters, `model_name` can be used to assign a different\n",
    "          name. Also, when saving an instance of the model, it will create a\n",
    "          directory using this parameters as its name and will be saved in\n",
    "          `output_dir`.\n",
    "\n",
    "        - `huggingface-path` (str) the name of the model in the hub of\n",
    "          huggingface. For example: `bert-base-uncased` or\n",
    "          `microsoft/deberta-v3-large`.\n",
    "\n",
    "        - `checkpoint-path` (str) [optional] path to a huggingface checkpoint\n",
    "        directory containing its configuration.\n",
    "\n",
    "        - `epochs` (int) number of epochs for training the transformer.\n",
    "\n",
    "        - `batch-size` (int) batch size used for training the transformer.\n",
    "\n",
    "        - `random_state` (int) integer number to initialize the random state\n",
    "          during the training process.\n",
    "\n",
    "        - `lr` (float) learning rate for training the transformer.\n",
    "\n",
    "        - `weight-decay` (float) weight decay penalty applied to the\n",
    "          transformer.\n",
    "\n",
    "        - `device` (str) Use `cpu` or `gpu`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 huggingface_path: str = \"google/mt5-base\",\n",
    "                 checkpoint_path: str = None,\n",
    "                 epochs: int = 2,\n",
    "                 batch_size: int = 16,\n",
    "                 random_state: int = 42,\n",
    "                 lr: float = 2e-5,\n",
    "                 weight_decay: float = 0.01,\n",
    "                 num_labels: int = 2,\n",
    "                 output_dir: str = \"./default_output_dir\",\n",
    "                 device: str = \"cpu\") -> None:\n",
    "        super(TransformerModel, self).__init__(output_dir)\n",
    "\n",
    "        # Load model from hugginface hub.\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            huggingface_path,\n",
    "            num_labels=num_labels,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        # Load tokenizer from huggingface hub.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(huggingface_path,\n",
    "                                                  do_lower_case=True)\n",
    "        # Set class attributes.\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.args = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def set_training_args(self):\n",
    "        self.args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            learning_rate=self.lr,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            num_train_epochs=self.epochs,\n",
    "            weight_decay=self.weight_decay,\n",
    "            seed=self.random_state,\n",
    "            #data_seed=self.random_state,\n",
    "            optim=\"adamw_hf\")\n",
    "\n",
    "    def tokenize(self, example: str):\n",
    "        \"\"\"\n",
    "        Tokenize a sentence using the model tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "    def build_loader(self, sentences: pd.Series, labels: pd.Series = None):\n",
    "        \"\"\"\n",
    "        Create a Dataset loader from huggingface tokenizing each sentence.\n",
    "\n",
    "        parameters:\n",
    "            - `sentences` (pd.Series[str])\n",
    "            - `labels` (pd.Series[int])\n",
    "        \"\"\"\n",
    "        dataset_dict = {\"text\": sentences}\n",
    "        if labels is not None:\n",
    "            dataset_dict.update({\"label\": labels})\n",
    "    \n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        return dataset.map(self.tokenize, batched=True)\n",
    "\n",
    "    def fit(self,\n",
    "            x_train: pd.Series,\n",
    "            y_train: pd.Series) -> None:\n",
    "        \"\"\"\n",
    "        Fit method that takes training text documents `x_train` and their labels\n",
    "        `y_train` and train a transformer based model. In this case the `x_dev`\n",
    "        and `y_dev` are used to evaluate the model in each epoch. When saving\n",
    "        the model, train and dev losses are saved too.\n",
    "\n",
    "        parameters:\n",
    "            - `x_train` (pd.Series[str]) training text documents.\n",
    "            - `y_train` (pd.Series[int]) training labels.\n",
    "            - `x_dev` (pd.Series[str]) dev text documents.\n",
    "            - `y_dev` (pd.Series[int]) dev labels.\n",
    "        \"\"\"\n",
    "        self.set_training_args()\n",
    "\n",
    "        # Create data collator.\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer,\n",
    "                                                padding=True)\n",
    "\n",
    "        train_size = int(0.8 * len(x_train))  # 80-20 split\n",
    "        train_sentences, eval_sentences = x_train[:train_size], x_train[train_size:]\n",
    "        train_labels, eval_labels = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "        # Create dataset loaders for train and eval sets.\n",
    "        train_dataset = self.build_loader(sentences=train_sentences, labels=train_labels)\n",
    "        eval_dataset = self.build_loader(sentences=eval_sentences, labels=eval_labels)\n",
    "\n",
    "        # Move huggingface model to the device indicated.\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Instance huggingface Trainer.\n",
    "        self.trainer = Trainer(model=self.model,\n",
    "                               args=self.args,\n",
    "                               train_dataset=train_dataset,\n",
    "                               eval_dataset=eval_dataset,\n",
    "                               tokenizer=self.tokenizer,\n",
    "                               data_collator=data_collator)\n",
    "\n",
    "        # If there is any checkpoint provided, training is resumed from it.\n",
    "        if self.checkpoint_path is not None:\n",
    "            self.trainer.train(self.checkpoint_path)\n",
    "        else:\n",
    "            self.trainer.train()\n",
    "\n",
    "    def predict_proba(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Estimate classification probabilities on samples in `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array of floats with n classes columns) probability\n",
    "              labels for sample `x`.\n",
    "        \"\"\"\n",
    "        # Use text classification pipeline to make predictions.\n",
    "        pipe = TextClassificationPipeline(model=self.model,\n",
    "                                          tokenizer=self.tokenizer,\n",
    "                                          return_all_scores=True,\n",
    "                                          framework=\"pt\")\n",
    "        preds = pipe(x.tolist())\n",
    "        y_prob = np.array([[pred[i][\"score\"] for i in range(self.num_labels)]\n",
    "                           for pred in preds])\n",
    "        return y_prob\n",
    "\n",
    "    def predict(self, x: pd.Series) -> np.array:\n",
    "        \"\"\"\n",
    "        Perform classification on samples in `x`.\n",
    "\n",
    "        parameters:\n",
    "            - `x` (pd.Series[str]) sample to predict.\n",
    "\n",
    "        returns:\n",
    "            - `y_pred` (np.array[int]) class labels for sample `x`.\n",
    "        \"\"\"\n",
    "        y_prob = predict_proba(x)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Save model weights and its configuration in `self.output_dir`. It\n",
    "        follows huggingface save standards so the model can be re-loaded using\n",
    "        huggingface `from_pretrained()` functionality.\n",
    "        \"\"\"\n",
    "        if self.trainer is not None:\n",
    "            os.makedirs(f\"{self.output_dir}/model\", exist_ok=True)\n",
    "            self.trainer.save_model(output_dir=f\"{self.output_dir}/model\")\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"Method ignored. Trying to save model without training it.\"\n",
    "                \"Please use `fit` before `save_model`\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "    def load_model(self, model_dirpath):\n",
    "        \"\"\"\n",
    "        Load model weights. It takes directory path `model_dirpath` where the\n",
    "        model necessary data is in.\n",
    "\n",
    "        parameters:\n",
    "            - `model_dirpath` (str) Directory path where the model is saved.\n",
    "        \"\"\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_dirpath)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dirpath)\n",
    "\n",
    "    def embed(self, x: pd.Series) -> np.array:\n",
    "        inputs = self.tokenizer(x.tolist(),\n",
    "                                truncation=True,\n",
    "                                max_length=256,\n",
    "                                padding=\"max_length\",\n",
    "                                return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # Get the last hidden state\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        # Get only the CLS token for each instance in `x` (the one used for classification).\n",
    "        # cls = last_hidden_states[:, 0, :]\n",
    "\n",
    "        # Detach Pytorch tensor to Numpy array.\n",
    "        return last_hidden_states.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4b8732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tweets  labels\n",
      "0              going through depression open this thread       1\n",
      "1      fivebalanceusa good afternoon would honored to...       0\n",
      "2                 fkin depressed gonna feeling like this       1\n",
      "3       when have anxiety every pleasure guilty pleasure       0\n",
      "4      sorry canxexxt talk right ixexxm doing girl sh...       1\n",
      "...                                                  ...     ...\n",
      "60167  bits quite exciting cheerful hear that some li...       0\n",
      "60168  bixexxm glad didnxexxt force myself like music...       0\n",
      "60169  actually leaving saturday happy really feeling...       0\n",
      "60170  correction plan will invest working people cre...       0\n",
      "60171  byoung randy came this figure incredible hand ...       0\n",
      "\n",
      "[60172 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data from CSV files\n",
    "arabic_data = pd.read_excel(\"newenglish.xlsx\",header=0)\n",
    "# dev_data = pd.read_csv(\"/kaggle/input/translated-datasets/malayalam_only_dev.csv\")\n",
    "# test_data = pd.read_csv(\"Dataset/Transliterated Only/Tamil/tamil_transliterated_test.csv\")\n",
    "print(arabic_data)\n",
    "arabic_data.columns = ['tweets', 'labels']\n",
    "\n",
    "# Remove any leading or trailing spaces from the labels\n",
    "# = arabic_data['label'].str.strip()\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "# Assuming you want to split it into 80% train and 20% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(arabic_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally, you can also split a development/validation set if needed\n",
    "# dev_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Again, ensure labels are stripped of any leading or trailing spaces\n",
    "# train_data['label'] = train_data['label'].str.strip()\n",
    "# test_data['label'] = test_data['label'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa2ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns = ['text', 'label']\n",
    "# dev_data.columns = ['text', 'label']\n",
    "test_data.columns = ['text', 'label']\n",
    "train_data['label'] = train_data['label']\n",
    "# dev_data['label'] = dev_data['label'].str.strip()\n",
    "test_data['label'] = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19564282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Step 2: Preprocess the data, separating sentences and labels\n",
    "x_train, y_train = train_data[\"text\"], train_data[\"label\"]\n",
    "# x_dev, y_dev = dev_data[\"text\"], dev_data[\"label\"]\n",
    "x_test, y_test = test_data[\"text\"], test_data[\"label\"]\n",
    "x_train = x_train.astype(str)\n",
    "x_test = x_test.astype(str)\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Step 3: Initialize the TransformerModel\n",
    "model = TransformerModel(huggingface_path=\"xlm-roberta-base\",\n",
    "                         epochs=2,\n",
    "                         batch_size=16,\n",
    "                         random_state=42,\n",
    "                         lr=2e-5,\n",
    "                         weight_decay=0.01,\n",
    "                         num_labels=2)\n",
    "\n",
    "# # Step 4: Train the model on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7bd8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_data['label'] = le.fit_transform(train_data['label'])\n",
    "# dev_data['label'] = le.transform(dev_data['label'])\n",
    "test_data['label'] = le.transform(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f4df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function TransformerModel.tokenize at 0x7fe579aba040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7be0112c8c74177a85999728dfac9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23c9b0141674989adc25264a76b680a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4814' max='4814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4814/4814 5:24:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.158131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.165378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./default_output_dir/checkpoint-2407 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./default_output_dir/checkpoint-4814 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac59f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output directory where you want to save the model\n",
    "output_dir = \"Models/English_xlmr.h5\"  # Replace this with your desired output directory\n",
    "\n",
    "# Set the output_dir in the model instance\n",
    "model.output_dir = output_dir\n",
    "\n",
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b6b57eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TransformerModel class\n",
    "model = TransformerModel()\n",
    "\n",
    "# Load the saved model from the specified directory\n",
    "model.load_model(\"Models/English_xlmr.h5/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90dc51cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|| 1505/1505 [3:12:01<00:00,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Sample sentences for demonstration\n",
    "sample_sentences = x_train  # Assuming x_train contains your sentences\n",
    "labels = y_train  # Assuming y_train contains your labels\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = (len(sample_sentences) + batch_size - 1) // batch_size\n",
    "\n",
    "# Initialize an empty list to store the embeddings and labels\n",
    "embeddings_list = []\n",
    "label_list = []\n",
    "\n",
    "# Use tqdm to create a progress bar\n",
    "for i in tqdm(range(num_batches), desc=\"Processing batches\", total=num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "\n",
    "    # Get embeddings for the current batch\n",
    "    batch_embeddings = model.embed(sample_sentences[start_idx:end_idx])\n",
    "\n",
    "    # Append the embeddings to the list\n",
    "    embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    # Append the corresponding labels to the label list\n",
    "    label_list.append(labels[start_idx:end_idx])\n",
    "\n",
    "# Concatenate embeddings and labels from all batches into a single numpy array.\n",
    "embeddings_list = np.concatenate(embeddings_list, axis=0)\n",
    "label_list = np.concatenate(label_list, axis=0)\n",
    "\n",
    "# Now, embeddings_list contains the embeddings for all sentences,\n",
    "# and label_list contains the corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f7ed5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48137, 256, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca18e4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48137,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5abfb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory = \"Embeddings/MT5/ENGLISH\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a8574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to a file\n",
    "# Embeddings/Muril/Tamil/Transliterated Only/tamil_muril_transliterated_train_embeds.npy\n",
    "np.save(\"Embeddings/MT5/ENGLISH/english_mt5_transliterated_train_embeds.npy\", embeddings_list)\n",
    "np.save(\"Embeddings/MT5/ENGLISH/english_mt5_transliterated_train_labels.npy\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ac8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
