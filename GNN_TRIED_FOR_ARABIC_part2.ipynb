{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2ZThrpX8mXkSQQJrP/3V2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vnavya2004/BTP/blob/main/GNN_TRIED_FOR_ARABIC_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Load dataset\n",
        "uploaded = files.upload()\n",
        "df = pd.read_excel(pd.ExcelFile(list(uploaded.keys())[0]), header=0)\n",
        "df = df.sample(frac=0.4, random_state=42)\n",
        "# Graph Preparation\n",
        "tweets_column = 'tweet'\n",
        "labels_column = 'label'\n",
        "NUM_LABELS = len(df[labels_column].unique())\n",
        "possible_labels = df[labels_column].unique()\n",
        "label_dict = {possible_label: index for index, possible_label in enumerate(possible_labels)}\n",
        "df['labels'] = df[labels_column].map(label_dict)\n",
        "\n",
        "# Split the dataset\n",
        "df_labeled, df_temp = train_test_split(df, stratify=df[labels_column], test_size=0.8)\n",
        "df_unlabeled, df_test = train_test_split(df_temp, stratify=df_temp[labels_column], test_size=0.25)\n"
      ],
      "metadata": {
        "id": "Lv0cJ8JEw504"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YksY8_6fsdoZ",
        "outputId": "8688c59f-a211-4fd9-ce58-8e84bee94225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Supervised Training Loss: 0.6938855648040771\n",
            "Consistency Loss: 8.85823192220414e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "Supervised Training Loss: 0.6932591795921326\n",
            "Consistency Loss: 1.8543927353675826e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "Supervised Training Loss: 0.6929776668548584\n",
            "Consistency Loss: 1.3403496268438175e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "Supervised Training Loss: 0.6928291916847229\n",
            "Consistency Loss: 5.5335672044520834e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "Supervised Training Loss: 0.6927558183670044\n",
            "Consistency Loss: 5.429991034588966e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6\n",
            "Supervised Training Loss: 0.6927223205566406\n",
            "Consistency Loss: 4.5242515511745296e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7\n",
            "Supervised Training Loss: 0.6927068829536438\n",
            "Consistency Loss: 8.765574932567688e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8\n",
            "Supervised Training Loss: 0.6927003264427185\n",
            "Consistency Loss: 7.354530850989249e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9\n",
            "Supervised Training Loss: 0.692698061466217\n",
            "Consistency Loss: 4.557443489261459e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10\n",
            "Supervised Training Loss: 0.692697286605835\n",
            "Consistency Loss: 2.765159301532094e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                              "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.485\n",
            "Testing F1 Score: 0.3168013468013468\n",
            "Testing Precision: 0.23522500000000002\n",
            "Testing Recall: 0.485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Correct the graph creation function to have proper feature size\n",
        "def create_graph_data(df, feature_dim=64):\n",
        "    num_nodes = len(df)\n",
        "\n",
        "    # Create random features for nodes as an example (Replace with actual features if available)\n",
        "    x = torch.randn(num_nodes, feature_dim, dtype=torch.float)  # Node features with 64 dimensions\n",
        "    labels = torch.tensor(df['labels'].values, dtype=torch.long)\n",
        "\n",
        "    # Sample graph: Replace this with your logic for creating edges between nodes\n",
        "    edge_index = torch.tensor([[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j], dtype=torch.long).t().contiguous()\n",
        "    data = Data(x=x, edge_index=edge_index, y=labels)\n",
        "    return data\n",
        "\n",
        "# Create graph data with corrected feature dimensions\n",
        "graph_data_labeled = create_graph_data(df_labeled, feature_dim=64)\n",
        "graph_data_unlabeled = create_graph_data(df_unlabeled, feature_dim=64)\n",
        "graph_data_test = create_graph_data(df_test, feature_dim=64)\n",
        "\n",
        "# DataLoader remains the same\n",
        "dataloader_train = DataLoader([graph_data_labeled], batch_size=batch_size, shuffle=True)\n",
        "dataloader_unlabeled = DataLoader([graph_data_unlabeled], batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader([graph_data_test], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define GNN Model with correct input dimensions\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_channels, num_classes):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the models with the corrected input feature size\n",
        "student_model = GNNModel(num_node_features=64, hidden_channels=64, num_classes=NUM_LABELS)\n",
        "teacher_model = GNNModel(num_node_features=64, hidden_channels=64, num_classes=NUM_LABELS)\n",
        "\n",
        "# Set up the device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "student_model.to(device)\n",
        "teacher_model.to(device)\n",
        "\n",
        "# Copy student model parameters to the teacher model\n",
        "teacher_model.load_state_dict(student_model.state_dict())\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "epochs = 10\n",
        "alpha = 0.999  # EMA decay rate\n",
        "\n",
        "# Define evaluation metrics\n",
        "def compute_metrics(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    accuracy = accuracy_score(labels_flat, preds_flat)\n",
        "    f1 = f1_score(labels_flat, preds_flat, average='weighted')\n",
        "    precision = precision_score(labels_flat, preds_flat, average='weighted')\n",
        "    recall = recall_score(labels_flat, preds_flat, average='weighted')\n",
        "    return accuracy, f1, precision, recall\n",
        "\n",
        "# Update teacher model using EMA of student model\n",
        "def update_teacher(student_model, teacher_model, alpha):\n",
        "    for student_param, teacher_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
        "        teacher_param.data = alpha * teacher_param.data + (1 - alpha) * student_param.data\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "    loss_train_total = 0\n",
        "    progress_bar = tqdm(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
        "\n",
        "    # Train the student on labeled data\n",
        "    for batch in progress_bar:\n",
        "        student_model.zero_grad()\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Forward pass through the student model\n",
        "        logits_student = student_model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(logits_student, batch.y)  # Supervised loss on labeled data\n",
        "\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item())})\n",
        "\n",
        "    # Consistency Loss on Unlabeled Data\n",
        "    loss_consistency_total = 0\n",
        "    for batch in DataLoader([graph_data_unlabeled], batch_size=batch_size, shuffle=True):\n",
        "        student_model.zero_grad()\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Predictions from student and teacher models\n",
        "        student_preds = student_model(batch.x, batch.edge_index)\n",
        "        teacher_preds = teacher_model(batch.x, batch.edge_index)\n",
        "\n",
        "        # Compute consistency loss (e.g., Mean Squared Error between predictions)\n",
        "        consistency_loss = F.mse_loss(student_preds, teacher_preds.detach())\n",
        "        loss_consistency_total += consistency_loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        consistency_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Update the teacher model using EMA\n",
        "    update_teacher(student_model, teacher_model, alpha)\n",
        "\n",
        "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
        "    loss_consistency_avg = loss_consistency_total / len(dataloader_unlabeled)\n",
        "\n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "    tqdm.write(f'Supervised Training Loss: {loss_train_avg}')\n",
        "    tqdm.write(f'Consistency Loss: {loss_consistency_avg}')\n",
        "\n",
        "# Evaluation on test data\n",
        "teacher_model.eval()\n",
        "loss_test_total = 0\n",
        "predictions, true_vals = [], []\n",
        "\n",
        "for batch in tqdm(dataloader_test, desc='Testing', leave=False):\n",
        "    batch = batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = teacher_model(batch.x, batch.edge_index)\n",
        "    loss = F.cross_entropy(outputs, batch.y)\n",
        "    loss_test_total += loss.item()\n",
        "\n",
        "    preds = outputs.detach().cpu().numpy()\n",
        "    labels = batch.y.cpu().numpy()\n",
        "\n",
        "    predictions.append(preds)\n",
        "    true_vals.append(labels)\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_vals = np.concatenate(true_vals, axis=0)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy, test_f1, test_precision, test_recall = compute_metrics(predictions, true_vals)\n",
        "print(f'Testing Accuracy: {test_accuracy}')\n",
        "print(f'Testing F1 Score: {test_f1}')\n",
        "print(f'Testing Precision: {test_precision}')\n",
        "print(f'Testing Recall: {test_recall}')\n"
      ]
    }
  ]
}