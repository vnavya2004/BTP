{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c183707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied: scikit-learn in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: transformers in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: datasets in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: fsspec in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: xxhash in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:482: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-11-12 17:06:00.487031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 17:06:00.836168: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 17:06:00.923678: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 17:06:02.481864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-12 17:06:02.482179: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-12 17:06:02.482201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                              tweet  label  \\\n",
      "2130        2239  ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø© ÙƒÙ†Øª Ø§Ø¹Ø§Ù†ÙŠ Ù…Ù†Ù‡ Ù‡Ù„Ø¹ Ùˆ Ø®ÙˆÙ Ù…Ù† Ø§Ù„Ù…ÙˆØª ÙÙ‚...      1   \n",
      "7974        3010                          Ù‡Ù…ÙˆØª Ù…Ù† Ø§Ù„ÙØ±Ø­Ù‡ Ø¹Ø´Ø§Ù† Ø¹Ù„ÙˆÙ„Ùˆ      0   \n",
      "7306        2339    Ø§Ù†Ø§ ÙØ±Ø­Ø§Ù† Ø¬Ø¯Ø§ Ø¨Ø±Ø¯ ÙØ¹Ù„ Ø§Ù„Ù†Ø§Ø³ Ø¹Ù„Ù‰ Ø¯ÙˆØ± Ø¹Ù… Ø­Ø³Ù† Ù...      0   \n",
      "1182        1214                                    Ù†ÙˆÙ… ÙˆØ§ÙƒÙ„ Ø§ÙƒØªØ¦Ø§Ø¨      1   \n",
      "7370        2403                     Ø§Ù†Ø§ ÙØ±Ø­Ø§Ù† Ø¬Ø¯Ø§ Ù„ Ø­ÙˆØ±ÙŠÙ‡ ÙØ±ØºÙ„ÙŠ â¤ï¸      0   \n",
      "...          ...                                                ...    ...   \n",
      "8871        3910              Ù‡Ù…ÙˆØª Ù…Ù† Ø§Ù„Ø¶Ø­Ùƒ Ø§Ù„Ù…Ø«Ù„ ÙÙŠ Ù…Ø­Ù„Ù‡ ØªÙ…Ø§Ù… ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚      0   \n",
      "9826        4865                Ø±Ø¨Ù†Ø§ ÙŠØ®Ù„Ù‰ Ø§Ù„Ù†Ø§Ø³ Ø§Ù„Ø­Ù„ÙˆØ© ÙÙ‰ Ø­ÙŠØ§ØªÙ†Ø§ .       0   \n",
      "5268         272  Ø±Ø¨ÙŠ Ø§Ø¬Ø¹Ù„ Ø§Ù„ÙŠÙˆÙ… Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„Ø£Ù…Ø³ ÙˆØºØ¯Ù‹Ø§ Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„ÙŠ...      0   \n",
      "9666        4705  Ù†ØµÙŠØ­Ø© Ø§Ù„Ø¹Ù…Ø±  Ù„Ù† ÙŠØªÙƒØ±Ø±  ÙÙ„Ø§ ØªØ®Ø³Ø± ÙˆÙ‚ØªÙƒ Ø¨Ø§Ù„Ø­Ù‚Ø¯ ÙˆØ§...      0   \n",
      "6090        1116                                Ø§Ù†Ø§ Ù…ØªÙØ§Ø¦Ù„ Ø¨ÙƒÙ„Ø§Ù…Ùƒ ğŸ‘      0   \n",
      "\n",
      "                                          tweet_english  \n",
      "2130  for a long time i suffered from panic and fear...  \n",
      "7974        im dying of happiness because of his height  \n",
      "7306  i am very happy with peoples reaction to the r...  \n",
      "1182                     sleeping and eating depression  \n",
      "7370               i am very happy for houria farghali   \n",
      "...                                                 ...  \n",
      "8871       im dying of laughter the proverb is perfect   \n",
      "9826         may god keep beautiful people in our lives  \n",
      "5268  my lord make today better than yesterday and t...  \n",
      "9666  the advice of a lifetime will not be repeated ...  \n",
      "6090                   i am optimistic about your words  \n",
      "\n",
      "[2000 rows x 4 columns]\n",
      "      Unnamed: 0                                              tweet  label  \\\n",
      "6252        1278  Ø¹ÙŠØ¯ Ù‚ÙŠØ§Ù…Ø© Ù…Ø¬ÙŠØ¯ ÙŠØ§ Ø´Ø¨Ø§Ø¨ Ø§ØªÙ…Ù†Ù‰ Ø§ÙŠØ§Ù…Ù†Ø§ ÙƒÙ„Ù‡Ø§ ØªØ¨Ù‚Ù‰ ...      0   \n",
      "4684        5141              Ø£Ù„ÙŠØ³ Ù…ÙÙ…Ù„Ù‹Ø§ Ø£Ù† ØªØ­Ø²Ù† Ù„Ù†ÙØ³ Ø§Ù„Ø³Ø¨Ø¨ ÙŠÙˆÙ…ÙŠØ§Ù‹      1   \n",
      "1731        1784  Ø§Ù†Ø§ Ù‡Ø§Ù„ÙŠÙˆÙ…ÙŠÙ† ØµØ§ÙŠØ± Ø¹Ù†Ø¯ÙŠ Ø¶ÙŠÙ‚ ÙÙŠ ØªÙ†ÙØ³ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ø·Ø¨...      1   \n",
      "4742        5201  Ù„Ù… ØªØ¹Ø¯ Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙˆØ§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù…Ù‡Ø±Ø¨ØŒ ÙˆÙ„Ø§ Ø§Ù„Ø£ØºØ§Ù†ÙŠ ÙˆØ§...      1   \n",
      "4521        4757  Ø®Ø°Ù„Ø§Ù†ÙŠ Ù„Ù… Ø£ØªÙˆÙ‚Ø¹ ÙŠÙˆÙ…Ø§ Ø£Ù† Ø£Ø®Ø°Ù„ Ø¨Ù‡Ø§Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„ Ø£Ø´Ø¹Ø± ...      1   \n",
      "...          ...                                                ...    ...   \n",
      "6412        1438    ÙŠØ§Ù„Ù„Ù‡ ÙˆØ§Ù„Ù„Ù‡ Ø§Ù†ÙŠ Ø³Ø¹ÙŠØ¯ Ø¨Ø¨Ù‡Ø¬ØªÙƒÙ… ÙŠØ§Ø§ Ø±Ø¨ Ø¯ÙˆÙˆÙˆÙ… Ù„Ø­...      0   \n",
      "8285        3322               Ù‚Ø³Ù… Ø¨Ø§Ù„Ù„Ù‡ Ù…Ù† Ø²Ù…Ø§Ù† Ù…Ø§ Ø´Ø§Ù‡Ù‚Øª Ù…Ù† Ø§Ù„ÙØ±Ø­Ù‡      0   \n",
      "7853        2889                        Ø·Ù…ÙˆØ­ Ø´ØºÙ Ø£Ù…Ù„ Ø­ÙŠØ§Ø© ÙˆØ³ÙˆÙ ØªØµÙ„       0   \n",
      "1095        1124  ÙˆÙ„Ù‡ Ù…Ø§ Ø§Ù†Ø§ Ø¹Ø§Ø±Ù Ù‡Ùˆ Ø¥ÙƒØªØ¦Ø§Ø¨ ÙˆÙ„Ø§ Ø§Ù†Ø·ÙØ§Ø¡ ÙˆÙ„Ø§ Ù†Ø¶Ø¬ Ùˆ...      1   \n",
      "6929        1957           Ø§Ø´Ø¹Ø± Ø¨Ø§Ù„Ø§Ø·Ù…Ø¦Ù†Ø§Ù† ÙˆØ§Ù†Ø§ Ù…Ø¹Ø§Ù‡ Ù‡Ùˆ Ø¯Ø© Ø§Ù„Ø­Ø¨ Ø¨Ù‚ÙŠ      0   \n",
      "\n",
      "                                          tweet_english  \n",
      "6252  merry christmas guys i hope all our days remai...  \n",
      "4684  isnt it boring to be sad for the same reason e...  \n",
      "1731  these two days i became abnormally short of br...  \n",
      "4742  movies and series are no longer an escape song...  \n",
      "4521  he let me down i never expected to be let down...  \n",
      "...                                                 ...  \n",
      "6412  oh god i swear to god i am happy with your joy...  \n",
      "8285  i swear to god i havent been so happy for a lo...  \n",
      "7853     ambition passion hope life and you will arrive  \n",
      "1095  what i dont know is depression extinction matu...  \n",
      "6929  i feel reassured when i am with him and this i...  \n",
      "\n",
      "[2000 rows x 4 columns]\n",
      "      Unnamed: 0                                              tweet  label  \\\n",
      "3188        3369  Ù… Ø§Ù‚Ø¯Ø± ÙÙŠÙ†ÙŠ Ø±Ù‡Ø§Ø¨ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆÙ‚Ø§Ø¹Ø¯ Ø§ØªØ¹Ø§Ù„Ø¬ Ù…Ù†Ù‡ \\r\\n...      1   \n",
      "8293        3330   ÙÙŠ Ø¹ÙŠÙÙˆÙ†Ùƒ Ø´ÙØª Ù…Ù† Ø§Ù„ÙØ±Ø­Ù‡ ÙƒØ«ÙŠØ±\\r\\nÙˆÙÙŠ ØºØ±Ø§Ù…Ùƒ Ø¶Ø§Ø¹...      0   \n",
      "1710        1763                            Ùˆ Ø§Ù„Ù‡ Ù…Ù†Ø¬Ø¯ ğŸ˜­ ÙƒÙ„Ù‡ Ø§ÙƒØªØ¦Ø§Ø¨      1   \n",
      "7510        2543                                             Ù…Ø¨Ø³ÙˆØ·       0   \n",
      "1461        1509              Ø§ÙƒØªØ¦Ø§Ø¨ ÙƒÙ„ Ø§Ù„ÙƒØ±Ù‡ Ø§Ù„Ø§Ø±Ø¶ÙŠÙ‡ Ù†Ø§ÙŠÙ…Ù‡ Ø§Ù„Ø§ Ø§Ù†Ø§      1   \n",
      "...          ...                                                ...    ...   \n",
      "5076          76       Ø£ÙƒØ«Ø± Ø§Ù„Ù†Ø§Ø³ Ø³Ø¹Ø§Ø¯Ø© Ù…Ù† Ù„Ø§ ÙŠÙ†ØªØ¸Ø± Ø´ÙŠØ¦Ù‹Ø§ Ù…ÙÙ† Ø£Ø­Ø¯ ğŸ˜Œ      0   \n",
      "2518        2647                                    ÙˆØ§Ù„Ù„Ù‡ Ø§Ù†Ù‡Ø§ ÙƒØ¢Ø¨Ù‡      1   \n",
      "3664        3861            ÙÙÙƒØ±Ø© Ø§Ù„Ø¥Ù†ØªÙØ­ÙØ§Ø± ØªÙØ±Ø§ÙˆÙØ¯Ù†Ù‘ÙŠ ÙƒÙÙ„ Ù„ÙÙŠÙ’Ù„ÙÙ‡      1   \n",
      "1143        1172  Ù‡Ù„ ØªØ¹Ù„Ù… Ù„Ù…Ø§Ø°Ø§ Ù†Ø­Ù† Ù†Ø´Ø¹Ø± Ø¨Ø§Ù„Ø¶ÙŠÙ‚ ÙˆØ§Ù„Ø­Ø²Ù† Ø£Ø­ÙŠØ§Ù†Ø§ Ø¯Ùˆ...      1   \n",
      "4417        4650  ğŸ”–Ù…Ù† Ø§Ù„Ø®Ø§Øµ \\r\\nØ§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙƒÙ„ Ø¹Ø§Ù… ÙˆØ§Ù†Øª Ø¨Ø®ÙŠØ± Ù…Ùƒ...      1   \n",
      "\n",
      "                                          tweet_english  \n",
      "3188  i cant believe i have social phobia and am sti...  \n",
      "8293  in your eyes i saw a lot of joy\\r\\nand in your...  \n",
      "1710         and god is upholstered  its all depression  \n",
      "7510                                              happy  \n",
      "1461    depression the whole planet is asleep except me  \n",
      "...                                                 ...  \n",
      "5076  the happiest people are those who do not expec...  \n",
      "2518                             i swear its depressing  \n",
      "3664  the idea of committing suicide haunts me every...  \n",
      "1143  do you know why we feel upset and sad sometime...  \n",
      "4417   from private \\r\\npeace be upon you happy new ...  \n",
      "\n",
      "[6000 rows x 4 columns]\n",
      "training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b8554927eb4dc08792b255e0a18ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e091dcc4aa4049851423b101f10d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcse210001032\u001b[0m (\u001b[33mcse210001032-iit-indore\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abcd/Navya/wandb/run-20241112_170614-5oahn834</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/5oahn834' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/5oahn834' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface/runs/5oahn834</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 2:54:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.155271</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.920810</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>0.953289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.172432</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.955513</td>\n",
       "      <td>0.976285</td>\n",
       "      <td>0.965787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.195612</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended.\n",
      "Iterative Data Harvesting started.\n",
      "2000\n",
      "7924\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 41:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.179673</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.960823</td>\n",
       "      <td>0.969368</td>\n",
       "      <td>0.965076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.226304</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.956395</td>\n",
       "      <td>0.975296</td>\n",
       "      <td>0.965753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.224004</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.956480</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.966764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7924\n",
      "7982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 29:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.267644</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.258307</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.955470</td>\n",
       "      <td>0.975296</td>\n",
       "      <td>0.965281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.251538</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.963654</td>\n",
       "      <td>0.969368</td>\n",
       "      <td>0.966502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7982\n",
      "7999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 30:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353677</td>\n",
       "      <td>0.956500</td>\n",
       "      <td>0.942584</td>\n",
       "      <td>0.973320</td>\n",
       "      <td>0.957705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.330137</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.964215</td>\n",
       "      <td>0.958498</td>\n",
       "      <td>0.961348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.315376</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.964427</td>\n",
       "      <td>0.962525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative Data Harvesting ended.\n",
      "Final Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91d25f2231e4a0bb366695b306d80dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 02:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model results:\n",
      "Accuracy: 0.9620\n",
      "Precision: 0.9606\n",
      "Recall: 0.9644\n",
      "F1 Score: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      Unnamed: 0                                              tweet  label  \\\n",
       " 2130        2239  ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø© ÙƒÙ†Øª Ø§Ø¹Ø§Ù†ÙŠ Ù…Ù†Ù‡ Ù‡Ù„Ø¹ Ùˆ Ø®ÙˆÙ Ù…Ù† Ø§Ù„Ù…ÙˆØª ÙÙ‚...      1   \n",
       " 7974        3010                          Ù‡Ù…ÙˆØª Ù…Ù† Ø§Ù„ÙØ±Ø­Ù‡ Ø¹Ø´Ø§Ù† Ø¹Ù„ÙˆÙ„Ùˆ      0   \n",
       " 7306        2339    Ø§Ù†Ø§ ÙØ±Ø­Ø§Ù† Ø¬Ø¯Ø§ Ø¨Ø±Ø¯ ÙØ¹Ù„ Ø§Ù„Ù†Ø§Ø³ Ø¹Ù„Ù‰ Ø¯ÙˆØ± Ø¹Ù… Ø­Ø³Ù† Ù...      0   \n",
       " 1182        1214                                    Ù†ÙˆÙ… ÙˆØ§ÙƒÙ„ Ø§ÙƒØªØ¦Ø§Ø¨      1   \n",
       " 7370        2403                     Ø§Ù†Ø§ ÙØ±Ø­Ø§Ù† Ø¬Ø¯Ø§ Ù„ Ø­ÙˆØ±ÙŠÙ‡ ÙØ±ØºÙ„ÙŠ â¤ï¸      0   \n",
       " ...          ...                                                ...    ...   \n",
       " 9392        4431  Ø¨Ù‚Ø§Ù„Ù†Ø§ Ø¹Ø´Ø±Ø§Øª Ø§Ù„Ø³Ù†ÙŠÙ† Ø¨Ù†ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„ØªØ£ÙŠÙŠØ¯ ÙˆØ§Ù„Ø´ØªÙŠÙ…...      0   \n",
       " 904          926  Ø£Ø±Ù‰ Ø§Ù„Ø³Ø­Ø± Ø·Ø§Ù‚Ù‡ Ø´Ø±ÙŠØ±Ù‡ ØªØ¸Ø±Ø¨ Ø£Ù…Ø§ÙƒÙ† Ù…Ø¹ÙŠÙ†Ù‡ Ø¨Ø§Ù„Ø¯Ù…Ø§Øº ...      1   \n",
       " 9133        4172                    Ù…Ø­Ø¯Ø´ ÙŠÙ‚Ø¯Ø± ÙŠØ´Ø§Ø±ÙƒÙŠÙ†ÙŠ ÙÙŠ Ø±Ø²Ù‚ÙŠ    .      0   \n",
       " 8855        3894               ÙŠÙ‚Ù„Ø¹ Ø§Ø¨Ùˆ Ø§Ù„Ø¶Ø­Ùƒ ÙŠØ§ Ù†Ø§Ø³ ÙˆØ´Ø°Ø§ Ø§Ù„Ø¹Ù‚ÙˆÙ„         0   \n",
       " 1972        2052  Ø§ÙŠÙ‡ ÙˆØ§Ù„Ù‡ ÙˆØ´ Ø°Ø§ Ø­Ø²Ù† Ø¨Ø³ Ù…Ø§Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù†Øª Ø§Ù...      1   \n",
       " \n",
       "                                           tweet_english  \n",
       " 2130  for a long time i suffered from panic and fear...  \n",
       " 7974        im dying of happiness because of his height  \n",
       " 7306  i am very happy with peoples reaction to the r...  \n",
       " 1182                     sleeping and eating depression  \n",
       " 7370               i am very happy for houria farghali   \n",
       " ...                                                 ...  \n",
       " 9392  we have been training for decades to support a...  \n",
       " 904   i see magic as an evil energy that strikes cer...  \n",
       " 9133             no one can share my livelihood with me  \n",
       " 8855   abu aldahk takes off o people with strange minds  \n",
       " 1972  what a shame but what about the application yo...  \n",
       " \n",
       " [7999 rows x 4 columns],\n",
       " {'eval_loss': 0.31537577509880066,\n",
       "  'eval_accuracy': 0.962,\n",
       "  'eval_precision': 0.9606299212598425,\n",
       "  'eval_recall': 0.9644268774703557,\n",
       "  'eval_f1': 0.9625246548323472,\n",
       "  'eval_runtime': 133.4248,\n",
       "  'eval_samples_per_second': 14.99,\n",
       "  'eval_steps_per_second': 1.874,\n",
       "  'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pandas scikit-learn transformers torch datasets\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score, recall_score\n",
    "\n",
    "# Step 1: Load and Preprocess Dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from a CSV file and check for required columns.\"\"\"\n",
    "    data = pd.read_excel(file_path)\n",
    "#     data = data.sample(frac=0.04, random_state=42)  # Random 40%\n",
    "\n",
    "    if 'tweet_english' not in data.columns:\n",
    "        raise KeyError(\"The dataset must contain a 'text' column with tweet data.\")\n",
    "    if 'label' not in data.columns:\n",
    "        print(\"Warning: No 'label' column found. Assuming this is the unlabeled data.\")\n",
    "        data['label'] = None  # Add a label column with NaN values if missing\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_text(df):\n",
    "    \"\"\"Preprocess text data.\"\"\"\n",
    "    df['tweet_english'] = df['tweet_english'].str.lower()\n",
    "    df['tweet_english'] = df['tweet_english'].str.replace(r\"http\\S+|www\\S+|https\\S+\", '', regex=True)  # remove URLs\n",
    "    df['tweet_english'] = df['tweet_english'].str.replace(r'\\@\\w+|\\#', '', regex=True)  # remove mentions and hashtags\n",
    "    df['tweet_english'] = df['tweet_english'].str.replace(r'[^A-Za-z\\s]', '', regex=True)  # remove non-alphanumeric chars\n",
    "    return df\n",
    "\n",
    "# Step 2: Define Labeled and Unlabeled Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, label_column='label', labeled_size=0.2, unlabeled_size=0.6, test_size=0.2):\n",
    "    \"\"\"Split dataset into labeled, unlabeled, and test data.\"\"\"\n",
    "    # Ensure the sum of labeled_size, unlabeled_size, and test_size equals 1\n",
    "    assert labeled_size + unlabeled_size + test_size == 1, \"The sizes must sum to 1.\"\n",
    "    \n",
    "    # Treat all data as unlabeled, including the rows with labels\n",
    "    \n",
    "    \n",
    "    # Split the data into train (80%) and test (20%) sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Now, from the train_data (80%), we select 20% as labeled data\n",
    "    labeled_data, unlabeled_data = train_test_split(train_data, test_size=0.75, random_state=42)\n",
    "    \n",
    "    return labeled_data,  test_data , unlabeled_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Initialize Model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    batch['tweet_english'] = [str(item) for item in batch['tweet_english']]  \n",
    "    return tokenizer(batch['tweet_english'], padding='max_length', truncation=True, max_length=128)\n",
    "# Step 4: Training Arguments\n",
    "def setup_training_args():\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Training\n",
    "# ... (your existing code) ...\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Trainin\n",
    "from datasets import Dataset  \n",
    "def train_model(train_data, eval_data, model, training_args):\n",
    "    # Convert DataFrames to Hugging Face Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,  # Use Hugging Face Datasets\n",
    "        eval_dataset=eval_dataset,    # Use Hugging Face Datasets\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "def predict_with_confidence(trainer, data, threshold=0.9, batch_size=16):  # Added batch_size parameter\n",
    "    \"\"\"Predicts labels for unlabeled data with high confidence.\"\"\"\n",
    "    # Check if 'data' has any rows\n",
    "    if data.empty:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    # and filter out any empty strings\n",
    "    valid_tweets = [str(item) for item in data['tweet_english'].tolist() if str(item).strip()]\n",
    "    \n",
    "    # Check if there are any valid tweets\n",
    "    if not valid_tweets:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "    \n",
    "    # Process data in batches to reduce memory usage\n",
    "    all_high_confidence_indices = []\n",
    "    for i in range(0, len(valid_tweets), batch_size):\n",
    "        batch_tweets = valid_tweets[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_tweets, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        high_confidence_indices = (probs.max(dim=1).values > threshold).nonzero(as_tuple=True)[0]\n",
    "        all_high_confidence_indices.extend(high_confidence_indices.cpu().numpy() + i)  # Adjust indices\n",
    "\n",
    "    # If no high confidence predictions, return empty DataFrame\n",
    "    if not all_high_confidence_indices:\n",
    "        return pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    return data.iloc[all_high_confidence_indices]\n",
    "\n",
    "def iterative_harvesting(trainer, labeled_data, unlabeled_data, iterations=1, threshold=0.9):\n",
    "    for iteration in range(iterations):\n",
    "        high_conf_data = predict_with_confidence(trainer, unlabeled_data, threshold)\n",
    "        print(len(labeled_data))\n",
    "        labeled_data = pd.concat([labeled_data, high_conf_data])\n",
    "        print(len(labeled_data))\n",
    "        unlabeled_data = unlabeled_data.drop(high_conf_data.index)\n",
    "        trainer.train()  # Retrain on expanded labeled data\n",
    "    return labeled_data, trainer\n",
    "\n",
    "# Step 7: Zero-Shot Learning Integration (optional)\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = [\"depression\", \"no depression\"]\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Evaluation Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)  # Get the predicted class labels\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "# Main Pipeline Function\n",
    "def evaluate_model(trainer, eval_data):\n",
    "    # Re-tokenize eval_data to ensure consistency\n",
    "#     print(\"a\")\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "#     print(\"b\")\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "#     print(\"c\")\n",
    "    # Run evaluation\n",
    "    final_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "#     print(\"s\")\n",
    "    return final_results\n",
    "\n",
    "def run_pipeline(file_path):\n",
    "    # Load and preprocess data\n",
    "    data = load_data(file_path)\n",
    "    data = preprocess_text(data)\n",
    "\n",
    "    # Define labeled and unlabeled datasets\n",
    "    train_data, eval_data, unlabeled_data = split_data(data)\n",
    "    \n",
    "    print(train_data)\n",
    "    print(eval_data)\n",
    "    print(unlabeled_data)\n",
    "    # Initialize training arguments and trainer\n",
    "    training_args = setup_training_args()\n",
    "    print(\"training started.\")\n",
    "    trainer = train_model(train_data, eval_data, model, training_args)\n",
    "    print(\"training ended.\")\n",
    "    # Iterative Data Harvesting\n",
    "    print(\"Iterative Data Harvesting started.\")\n",
    "    final_labeled_data, final_trainer = iterative_harvesting(trainer, train_data, unlabeled_data)\n",
    "    print(\"Iterative Data Harvesting ended.\")\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"Final Evaluation started.\")\n",
    "    \n",
    "    final_results = evaluate_model(final_trainer, eval_data)\n",
    "    print(\"Final model results:\")\n",
    "    print(f\"Accuracy: {final_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {final_results['eval_precision']:.4f}\")\n",
    "    print(f\"Recall: {final_results['eval_recall']:.4f}\")\n",
    "    print(f\"F1 Score: {final_results['eval_f1']:.4f}\")\n",
    "    return final_labeled_data, final_results\n",
    "\n",
    "# Run the pipeline with your dataset file path\n",
    "file_path = 'Arabic_Depression_10.000_Tweets_translated (2).xlsx'\n",
    "run_pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcc31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
