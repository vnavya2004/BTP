{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c183707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied: scikit-learn in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: transformers in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: datasets in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abcd/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: requests in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: jinja2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: fsspec in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: xxhash in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/abcd/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abcd/.local/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:482: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/utils/generic.py:339: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-11-12 17:06:00.487031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 17:06:00.836168: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 17:06:00.923678: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 17:06:02.481864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-12 17:06:02.482179: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-10.0/lib64/stubs:/usr/local/cuda-10.0/lib64:::\n",
      "2024-11-12 17:06:02.482201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                              tweet  label  \\\n",
      "2130        2239  فترة طويلة كنت اعاني منه هلع و خوف من الموت فق...      1   \n",
      "7974        3010                          هموت من الفرحه عشان علولو      0   \n",
      "7306        2339    انا فرحان جدا برد فعل الناس على دور عم حسن ف...      0   \n",
      "1182        1214                                    نوم واكل اكتئاب      1   \n",
      "7370        2403                     انا فرحان جدا ل حوريه فرغلي ❤️      0   \n",
      "...          ...                                                ...    ...   \n",
      "8871        3910              هموت من الضحك المثل في محله تمام 😂😂😂😂      0   \n",
      "9826        4865                ربنا يخلى الناس الحلوة فى حياتنا .       0   \n",
      "5268         272  ربي اجعل اليوم أفضل من الأمس وغدًا أفضل من الي...      0   \n",
      "9666        4705  نصيحة العمر  لن يتكرر  فلا تخسر وقتك بالحقد وا...      0   \n",
      "6090        1116                                انا متفائل بكلامك 👍      0   \n",
      "\n",
      "                                          tweet_english  \n",
      "2130  for a long time i suffered from panic and fear...  \n",
      "7974        im dying of happiness because of his height  \n",
      "7306  i am very happy with peoples reaction to the r...  \n",
      "1182                     sleeping and eating depression  \n",
      "7370               i am very happy for houria farghali   \n",
      "...                                                 ...  \n",
      "8871       im dying of laughter the proverb is perfect   \n",
      "9826         may god keep beautiful people in our lives  \n",
      "5268  my lord make today better than yesterday and t...  \n",
      "9666  the advice of a lifetime will not be repeated ...  \n",
      "6090                   i am optimistic about your words  \n",
      "\n",
      "[2000 rows x 4 columns]\n",
      "      Unnamed: 0                                              tweet  label  \\\n",
      "6252        1278  عيد قيامة مجيد يا شباب اتمنى ايامنا كلها تبقى ...      0   \n",
      "4684        5141              أليس مُملًا أن تحزن لنفس السبب يومياً      1   \n",
      "1731        1784  انا هاليومين صاير عندي ضيق في تنفس بشكل غير طب...      1   \n",
      "4742        5201  لم تعد الأفلام والمسلسلات مهرب، ولا الأغاني وا...      1   \n",
      "4521        4757  خذلاني لم أتوقع يوما أن أخذل بهاذا الشكل أشعر ...      1   \n",
      "...          ...                                                ...    ...   \n",
      "6412        1438    يالله والله اني سعيد ببهجتكم ياا رب دوووم لح...      0   \n",
      "8285        3322               قسم بالله من زمان ما شاهقت من الفرحه      0   \n",
      "7853        2889                        طموح شغف أمل حياة وسوف تصل       0   \n",
      "1095        1124  وله ما انا عارف هو إكتئاب ولا انطفاء ولا نضج و...      1   \n",
      "6929        1957           اشعر بالاطمئنان وانا معاه هو دة الحب بقي      0   \n",
      "\n",
      "                                          tweet_english  \n",
      "6252  merry christmas guys i hope all our days remai...  \n",
      "4684  isnt it boring to be sad for the same reason e...  \n",
      "1731  these two days i became abnormally short of br...  \n",
      "4742  movies and series are no longer an escape song...  \n",
      "4521  he let me down i never expected to be let down...  \n",
      "...                                                 ...  \n",
      "6412  oh god i swear to god i am happy with your joy...  \n",
      "8285  i swear to god i havent been so happy for a lo...  \n",
      "7853     ambition passion hope life and you will arrive  \n",
      "1095  what i dont know is depression extinction matu...  \n",
      "6929  i feel reassured when i am with him and this i...  \n",
      "\n",
      "[2000 rows x 4 columns]\n",
      "      Unnamed: 0                                              tweet  label  \\\n",
      "3188        3369  م اقدر فيني رهاب اجتماعي وقاعد اتعالج منه \\r\\n...      1   \n",
      "8293        3330   في عيُونك شفت من الفرحه كثير\\r\\nوفي غرامك ضاع...      0   \n",
      "1710        1763                            و اله منجد 😭 كله اكتئاب      1   \n",
      "7510        2543                                             مبسوط       0   \n",
      "1461        1509              اكتئاب كل الكره الارضيه نايمه الا انا      1   \n",
      "...          ...                                                ...    ...   \n",
      "5076          76       أكثر الناس سعادة من لا ينتظر شيئًا مِن أحد 😌      0   \n",
      "2518        2647                                    والله انها كآبه      1   \n",
      "3664        3861            فِكرة الإنتِحَار تُراوِدنّي كُل لَيْلَه      1   \n",
      "1143        1172  هل تعلم لماذا نحن نشعر بالضيق والحزن أحيانا دو...      1   \n",
      "4417        4650  🔖من الخاص \\r\\nالسلام عليكم كل عام وانت بخير مك...      1   \n",
      "\n",
      "                                          tweet_english  \n",
      "3188  i cant believe i have social phobia and am sti...  \n",
      "8293  in your eyes i saw a lot of joy\\r\\nand in your...  \n",
      "1710         and god is upholstered  its all depression  \n",
      "7510                                              happy  \n",
      "1461    depression the whole planet is asleep except me  \n",
      "...                                                 ...  \n",
      "5076  the happiest people are those who do not expec...  \n",
      "2518                             i swear its depressing  \n",
      "3664  the idea of committing suicide haunts me every...  \n",
      "1143  do you know why we feel upset and sad sometime...  \n",
      "4417   from private \\r\\npeace be upon you happy new ...  \n",
      "\n",
      "[6000 rows x 4 columns]\n",
      "training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcd/anaconda3/envs/abcd/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b8554927eb4dc08792b255e0a18ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e091dcc4aa4049851423b101f10d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcse210001032\u001b[0m (\u001b[33mcse210001032-iit-indore\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abcd/Navya/wandb/run-20241112_170614-5oahn834</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/5oahn834' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cse210001032-iit-indore/huggingface/runs/5oahn834' target=\"_blank\">https://wandb.ai/cse210001032-iit-indore/huggingface/runs/5oahn834</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 2:54:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.155271</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.920810</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>0.953289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.172432</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.955513</td>\n",
       "      <td>0.976285</td>\n",
       "      <td>0.965787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.195612</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended.\n",
      "Iterative Data Harvesting started.\n",
      "2000\n",
      "7924\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 41:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.179673</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.960823</td>\n",
       "      <td>0.969368</td>\n",
       "      <td>0.965076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.226304</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.956395</td>\n",
       "      <td>0.975296</td>\n",
       "      <td>0.965753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.224004</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.956480</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.966764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7924\n",
      "7982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 29:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.267644</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.258307</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.955470</td>\n",
       "      <td>0.975296</td>\n",
       "      <td>0.965281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.251538</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.963654</td>\n",
       "      <td>0.969368</td>\n",
       "      <td>0.966502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7982\n",
      "7999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 30:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353677</td>\n",
       "      <td>0.956500</td>\n",
       "      <td>0.942584</td>\n",
       "      <td>0.973320</td>\n",
       "      <td>0.957705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.330137</td>\n",
       "      <td>0.961000</td>\n",
       "      <td>0.964215</td>\n",
       "      <td>0.958498</td>\n",
       "      <td>0.961348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.315376</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.964427</td>\n",
       "      <td>0.962525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative Data Harvesting ended.\n",
      "Final Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91d25f2231e4a0bb366695b306d80dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 02:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model results:\n",
      "Accuracy: 0.9620\n",
      "Precision: 0.9606\n",
      "Recall: 0.9644\n",
      "F1 Score: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      Unnamed: 0                                              tweet  label  \\\n",
       " 2130        2239  فترة طويلة كنت اعاني منه هلع و خوف من الموت فق...      1   \n",
       " 7974        3010                          هموت من الفرحه عشان علولو      0   \n",
       " 7306        2339    انا فرحان جدا برد فعل الناس على دور عم حسن ف...      0   \n",
       " 1182        1214                                    نوم واكل اكتئاب      1   \n",
       " 7370        2403                     انا فرحان جدا ل حوريه فرغلي ❤️      0   \n",
       " ...          ...                                                ...    ...   \n",
       " 9392        4431  بقالنا عشرات السنين بنتدرب على التأييد والشتيم...      0   \n",
       " 904          926  أرى السحر طاقه شريره تظرب أماكن معينه بالدماغ ...      1   \n",
       " 9133        4172                    محدش يقدر يشاركيني في رزقي    .      0   \n",
       " 8855        3894               يقلع ابو الضحك يا ناس وشذا العقول         0   \n",
       " 1972        2052  ايه واله وش ذا حزن بس ماعليه من التطبيق انت اف...      1   \n",
       " \n",
       "                                           tweet_english  \n",
       " 2130  for a long time i suffered from panic and fear...  \n",
       " 7974        im dying of happiness because of his height  \n",
       " 7306  i am very happy with peoples reaction to the r...  \n",
       " 1182                     sleeping and eating depression  \n",
       " 7370               i am very happy for houria farghali   \n",
       " ...                                                 ...  \n",
       " 9392  we have been training for decades to support a...  \n",
       " 904   i see magic as an evil energy that strikes cer...  \n",
       " 9133             no one can share my livelihood with me  \n",
       " 8855   abu aldahk takes off o people with strange minds  \n",
       " 1972  what a shame but what about the application yo...  \n",
       " \n",
       " [7999 rows x 4 columns],\n",
       " {'eval_loss': 0.31537577509880066,\n",
       "  'eval_accuracy': 0.962,\n",
       "  'eval_precision': 0.9606299212598425,\n",
       "  'eval_recall': 0.9644268774703557,\n",
       "  'eval_f1': 0.9625246548323472,\n",
       "  'eval_runtime': 133.4248,\n",
       "  'eval_samples_per_second': 14.99,\n",
       "  'eval_steps_per_second': 1.874,\n",
       "  'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pandas scikit-learn transformers torch datasets\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score, recall_score\n",
    "\n",
    "# Step 1: Load and Preprocess Dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from a CSV file and check for required columns.\"\"\"\n",
    "    data = pd.read_excel(file_path)\n",
    "#     data = data.sample(frac=0.04, random_state=42)  # Random 40%\n",
    "\n",
    "    if 'tweet_english' not in data.columns:\n",
    "        raise KeyError(\"The dataset must contain a 'text' column with tweet data.\")\n",
    "    if 'label' not in data.columns:\n",
    "        print(\"Warning: No 'label' column found. Assuming this is the unlabeled data.\")\n",
    "        data['label'] = None  # Add a label column with NaN values if missing\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_text(df):\n",
    "    \"\"\"Preprocess text data.\"\"\"\n",
    "    df['tweet_english'] = df['tweet_english'].str.lower()\n",
    "    df['tweet_english'] = df['tweet_english'].str.replace(r\"http\\S+|www\\S+|https\\S+\", '', regex=True)  # remove URLs\n",
    "    df['tweet_english'] = df['tweet_english'].str.replace(r'\\@\\w+|\\#', '', regex=True)  # remove mentions and hashtags\n",
    "    df['tweet_english'] = df['tweet_english'].str.replace(r'[^A-Za-z\\s]', '', regex=True)  # remove non-alphanumeric chars\n",
    "    return df\n",
    "\n",
    "# Step 2: Define Labeled and Unlabeled Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, label_column='label', labeled_size=0.2, unlabeled_size=0.6, test_size=0.2):\n",
    "    \"\"\"Split dataset into labeled, unlabeled, and test data.\"\"\"\n",
    "    # Ensure the sum of labeled_size, unlabeled_size, and test_size equals 1\n",
    "    assert labeled_size + unlabeled_size + test_size == 1, \"The sizes must sum to 1.\"\n",
    "    \n",
    "    # Treat all data as unlabeled, including the rows with labels\n",
    "    \n",
    "    \n",
    "    # Split the data into train (80%) and test (20%) sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Now, from the train_data (80%), we select 20% as labeled data\n",
    "    labeled_data, unlabeled_data = train_test_split(train_data, test_size=0.75, random_state=42)\n",
    "    \n",
    "    return labeled_data,  test_data , unlabeled_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Initialize Model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    batch['tweet_english'] = [str(item) for item in batch['tweet_english']]  \n",
    "    return tokenizer(batch['tweet_english'], padding='max_length', truncation=True, max_length=128)\n",
    "# Step 4: Training Arguments\n",
    "def setup_training_args():\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Training\n",
    "# ... (your existing code) ...\n",
    "\n",
    "# Step 5: Initialize Trainer and Model Trainin\n",
    "from datasets import Dataset  \n",
    "def train_model(train_data, eval_data, model, training_args):\n",
    "    # Convert DataFrames to Hugging Face Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,  # Use Hugging Face Datasets\n",
    "        eval_dataset=eval_dataset,    # Use Hugging Face Datasets\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "# Step 6: Iterative Data Harvesting - Semi-Supervised Learning\n",
    "def predict_with_confidence(trainer, data, threshold=0.9, batch_size=16):  # Added batch_size parameter\n",
    "    \"\"\"Predicts labels for unlabeled data with high confidence.\"\"\"\n",
    "    # Check if 'data' has any rows\n",
    "    if data.empty:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "\n",
    "    # Convert each item in the 'tweets' list to string using list comprehension\n",
    "    # and filter out any empty strings\n",
    "    valid_tweets = [str(item) for item in data['tweet_english'].tolist() if str(item).strip()]\n",
    "    \n",
    "    # Check if there are any valid tweets\n",
    "    if not valid_tweets:\n",
    "        return pd.DataFrame(columns=data.columns)  # Return empty DataFrame\n",
    "    \n",
    "    # Process data in batches to reduce memory usage\n",
    "    all_high_confidence_indices = []\n",
    "    for i in range(0, len(valid_tweets), batch_size):\n",
    "        batch_tweets = valid_tweets[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_tweets, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        high_confidence_indices = (probs.max(dim=1).values > threshold).nonzero(as_tuple=True)[0]\n",
    "        all_high_confidence_indices.extend(high_confidence_indices.cpu().numpy() + i)  # Adjust indices\n",
    "\n",
    "    # If no high confidence predictions, return empty DataFrame\n",
    "    if not all_high_confidence_indices:\n",
    "        return pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    return data.iloc[all_high_confidence_indices]\n",
    "\n",
    "def iterative_harvesting(trainer, labeled_data, unlabeled_data, iterations=1, threshold=0.9):\n",
    "    for iteration in range(iterations):\n",
    "        high_conf_data = predict_with_confidence(trainer, unlabeled_data, threshold)\n",
    "        print(len(labeled_data))\n",
    "        labeled_data = pd.concat([labeled_data, high_conf_data])\n",
    "        print(len(labeled_data))\n",
    "        unlabeled_data = unlabeled_data.drop(high_conf_data.index)\n",
    "        trainer.train()  # Retrain on expanded labeled data\n",
    "    return labeled_data, trainer\n",
    "\n",
    "# Step 7: Zero-Shot Learning Integration (optional)\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = [\"depression\", \"no depression\"]\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Evaluation Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)  # Get the predicted class labels\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "# Main Pipeline Function\n",
    "def evaluate_model(trainer, eval_data):\n",
    "    # Re-tokenize eval_data to ensure consistency\n",
    "#     print(\"a\")\n",
    "    eval_dataset = Dataset.from_pandas(eval_data.reset_index(drop=True))\n",
    "#     print(\"b\")\n",
    "    eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "#     print(\"c\")\n",
    "    # Run evaluation\n",
    "    final_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "#     print(\"s\")\n",
    "    return final_results\n",
    "\n",
    "def run_pipeline(file_path):\n",
    "    # Load and preprocess data\n",
    "    data = load_data(file_path)\n",
    "    data = preprocess_text(data)\n",
    "\n",
    "    # Define labeled and unlabeled datasets\n",
    "    train_data, eval_data, unlabeled_data = split_data(data)\n",
    "    \n",
    "    print(train_data)\n",
    "    print(eval_data)\n",
    "    print(unlabeled_data)\n",
    "    # Initialize training arguments and trainer\n",
    "    training_args = setup_training_args()\n",
    "    print(\"training started.\")\n",
    "    trainer = train_model(train_data, eval_data, model, training_args)\n",
    "    print(\"training ended.\")\n",
    "    # Iterative Data Harvesting\n",
    "    print(\"Iterative Data Harvesting started.\")\n",
    "    final_labeled_data, final_trainer = iterative_harvesting(trainer, train_data, unlabeled_data)\n",
    "    print(\"Iterative Data Harvesting ended.\")\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"Final Evaluation started.\")\n",
    "    \n",
    "    final_results = evaluate_model(final_trainer, eval_data)\n",
    "    print(\"Final model results:\")\n",
    "    print(f\"Accuracy: {final_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {final_results['eval_precision']:.4f}\")\n",
    "    print(f\"Recall: {final_results['eval_recall']:.4f}\")\n",
    "    print(f\"F1 Score: {final_results['eval_f1']:.4f}\")\n",
    "    return final_labeled_data, final_results\n",
    "\n",
    "# Run the pipeline with your dataset file path\n",
    "file_path = 'Arabic_Depression_10.000_Tweets_translated (2).xlsx'\n",
    "run_pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcc31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
